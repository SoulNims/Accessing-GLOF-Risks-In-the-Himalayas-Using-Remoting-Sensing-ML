














import pandas as pd
import numpy as np



pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
uncleaned_ml_combined=pd.read_csv("../CSVs/uncleaned_ml_combined.csv",index_col=False)


uncleaned_ml_combined.head()



uncleaned_ml_combined.info()


uncleaned_ml_neg=pd.read_csv("../CSVs/uncleaned_ml_neg.csv")
uncleaned_ml_neg.info()


uncleaned_ml_neg_1=pd.read_csv("../CSVs/uncleaned_ml_neg_1.csv")
uncleaned_ml_neg_1.info()


uncleaned_ml_pos=pd.read_csv("../CSVs/uncleaned_ml_pos.csv")





ml_neg_y_drop=uncleaned_ml_neg.drop(columns=['Year_final'])
ml_neg_y_drop


ml_pos_y_drop=uncleaned_ml_pos.drop(columns=['Year_final'])
ml_pos_y_drop


uncleaned_ml_combined=pd.read_csv('../CSVs/uncleaned_ml_combined.csv')
uncleaned_ml_combined.info()


ml_comb_y_drop=uncleaned_ml_combined.drop(columns=['Year_final'])
ml_comb_y_drop.head()


ml_neg_y_drop.to_csv("../CSVs/ml_neg_y_drop.csv",index=False)
ml_pos_y_drop.to_csv("../CSVs/ml_pos_y_drop.csv",index=False)
ml_comb_y_drop.to_csv("../CSVs/ml_comb_y_drop.csv",index=False)


ml_pos_y_drop.isnull().mean()*100


ml_neg_y_drop.isnull().mean()*100


ml_comb_y_drop.isnull().mean()*100


ml_pos_y_drop.head()





# Load Data
df_comb = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")
df_pos = pd.read_csv("../CSVs/uncleaned_ml_pos.csv")
# Complete Case Analysis (CCA)
df_comb = df.dropna()


print(df_comb.shape)
print(df_pos.shape)


df_pos.head()


# Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier


# Load Data
df_comb = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Complete Case Analysis (CCA)
df_comb = df.dropna()

# Features and Target
X = df_comb.drop("GLOF", axis=1)
y = df_comb["GLOF"]

# Preprocessing
# Only one-hot encode Lake_type_simplified,
# all others are numeric (including 0/1 binary flags)
cat_features = ["Lake_type_simplified"]
exclude_cols = cat_features + ["Year_final","Latitude", "Longitude"]
num_features = [col for col in X.columns if col not in exclude_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features)
    ]
)

# Ensemble Creation Function
# Undersample majority class into many subsets, train separate classifiers

def create_ensemble(X_train, y_train, n_models=5, base_model=None):
    """
    Create an ensemble of classifiers with undersampling.
    
    Parameters:
    - X_train, y_train: training data
    - n_models: number of models in ensemble
    - base_model: the classifier to use (e.g., RandomForestClassifier(), LogisticRegression(), XGBClassifier())
    """
    if base_model is None:
        base_model = RandomForestClassifier()  # default

    models = []

    # Split into minority (1) and majority (0)
    X_minority = X_train[y_train == 1]
    y_minority = y_train[y_train == 1]
    X_majority = X_train[y_train == 0]
    y_majority = y_train[y_train == 0]

    for i in range(n_models):
        # Undersample majority class
        X_majority_resampled, y_majority_resampled = resample(
            X_majority, y_majority,
            replace=False,
            n_samples=len(y_minority),
            random_state=i
        )

        # Combine resampled majority + minority
        X_balanced = pd.concat([X_majority_resampled, X_minority])
        y_balanced = pd.concat([y_majority_resampled, y_minority])

        # Clone base_model so each one is fresh
        clf = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", clone(base_model).set_params(random_state=i))
            if hasattr(base_model, "random_state") else
            ("classifier", clone(base_model))
        ])

        # Train
        clf.fit(X_balanced, y_balanced)
        models.append(clf)

    return models

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# ----------------------------
# Helper: Ensemble Prediction
# ----------------------------
def ensemble_predict(models, X):
    preds = np.array([m.predict(X) for m in models])
    # Majority vote: if at least half the models predict 1, label as 1 (else 0)
    final_preds = (np.sum(preds, axis=0) >= (len(models) / 2)).astype(int)
    return final_preds



import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
def evaluate_ensemble(name, ensemble_models, X_test, y_test, plot=True):
    # Predictions
    y_pred = ensemble_predict(ensemble_models, X_test)

    # Classification report
    print(f"\n=== {name} Ensemble: Classification Report ===")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    print(f"\n=== {name} Ensemble: Confusion Matrix ===")
    print(confusion_matrix(y_test, y_pred))

    # ROC-AUC (use predicted probabilities)
    y_probas = np.mean([m.predict_proba(X_test)[:, 1] for m in ensemble_models], axis=0)
    auc_score = roc_auc_score(y_test, y_probas)
    print(f"\n{name} Ensemble: ROC-AUC = {auc_score:.4f}")

    # Plot ROC curve
    if plot:
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.figure(figsize=(7,5))
        plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color="blue")
        plt.plot([0, 1], [0, 1], "k--", label="Chance")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve - {name} Ensemble")
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()



# ----------------------------
# Train-Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# Random Forest Ensemble
# ----------------------------
rf_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=RandomForestClassifier()
)
evaluate_ensemble("Random Forest", rf_models, X_test, y_test)


# ----------------------------
# Logistic Regression Ensemble
# ----------------------------
lr_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=LogisticRegression(max_iter=1000)
)
evaluate_ensemble("Logistic Regression", lr_models, X_test, y_test)


# ----------------------------
# XGBoost Ensemble
# ----------------------------
xgb_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=XGBClassifier(
        eval_metric="logloss"
    )
)
evaluate_ensemble("XGBoost", xgb_models, X_test, y_test)

















