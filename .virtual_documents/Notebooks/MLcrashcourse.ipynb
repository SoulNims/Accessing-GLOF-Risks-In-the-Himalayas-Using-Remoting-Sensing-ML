














import pandas as pd
import numpy as np



pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
uncleaned_ml_combined=pd.read_csv("../CSVs/uncleaned_ml_combined.csv",index_col=False)


uncleaned_ml_combined.head()



uncleaned_ml_combined.info()


uncleaned_ml_neg=pd.read_csv("../CSVs/uncleaned_ml_neg.csv")
uncleaned_ml_neg.info()


uncleaned_ml_neg_1=pd.read_csv("../CSVs/uncleaned_ml_neg_1.csv")
uncleaned_ml_neg_1.info()


uncleaned_ml_pos=pd.read_csv("../CSVs/uncleaned_ml_pos.csv")





ml_neg_y_drop=uncleaned_ml_neg.drop(columns=['Year_final'])
ml_neg_y_drop


ml_pos_y_drop=uncleaned_ml_pos.drop(columns=['Year_final'])
ml_pos_y_drop


uncleaned_ml_combined=pd.read_csv('../CSVs/uncleaned_ml_combined.csv')
uncleaned_ml_combined.info()


ml_comb_y_drop=uncleaned_ml_combined.drop(columns=['Year_final'])
ml_comb_y_drop.head()


ml_neg_y_drop.to_csv("../CSVs/ml_neg_y_drop.csv",index=False)
ml_pos_y_drop.to_csv("../CSVs/ml_pos_y_drop.csv",index=False)
ml_comb_y_drop.to_csv("../CSVs/ml_comb_y_drop.csv",index=False)


ml_pos_y_drop.isnull().mean()*100


ml_neg_y_drop.isnull().mean()*100


ml_comb_y_drop.isnull().mean()*100


ml_pos_y_drop.head()





# Load Data
df_comb = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")
df_pos = pd.read_csv("../CSVs/uncleaned_ml_pos.csv")
# Complete Case Analysis (CCA)
df_comb = df.dropna()


print(df_comb.shape)
print(df_pos.shape)


df_pos.head()


# Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier


# Load Data
df_comb = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Complete Case Analysis (CCA)
df_comb = df.dropna()

# Features and Target
X = df_comb.drop("GLOF", axis=1)
y = df_comb["GLOF"]

# Preprocessing
# Only one-hot encode Lake_type_simplified,
# all others are numeric (including 0/1 binary flags)
cat_features = ["Lake_type_simplified"]
exclude_cols = cat_features + ["Year_final","Latitude", "Longitude"]
num_features = [col for col in X.columns if col not in exclude_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features)
    ]
)

# Ensemble Creation Function
# Undersample majority class into many subsets, train separate classifiers

def create_ensemble(X_train, y_train, n_models=5, base_model=None):
    """
    Create an ensemble of classifiers with undersampling.
    
    Parameters:
    - X_train, y_train: training data
    - n_models: number of models in ensemble
    - base_model: the classifier to use (e.g., RandomForestClassifier(), LogisticRegression(), XGBClassifier())
    """
    if base_model is None:
        base_model = RandomForestClassifier()  # default

    models = []

    # Split into minority (1) and majority (0)
    X_minority = X_train[y_train == 1]
    y_minority = y_train[y_train == 1]
    X_majority = X_train[y_train == 0]
    y_majority = y_train[y_train == 0]

    for i in range(n_models):
        # Undersample majority class
        X_majority_resampled, y_majority_resampled = resample(
            X_majority, y_majority,
            replace=False,
            n_samples=len(y_minority),
            random_state=i
        )

        # Combine resampled majority + minority
        X_balanced = pd.concat([X_majority_resampled, X_minority])
        y_balanced = pd.concat([y_majority_resampled, y_minority])

        # Clone base_model so each one is fresh
        clf = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", clone(base_model).set_params(random_state=i))
            if hasattr(base_model, "random_state") else
            ("classifier", clone(base_model))
        ])

        # Train
        clf.fit(X_balanced, y_balanced)
        models.append(clf)

    return models

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Helper: Ensemble Prediction
def ensemble_predict(models, X):
    preds = np.array([m.predict(X) for m in models])
    # Majority vote: if at least half the models predict 1, label as 1 (else 0)
    final_preds = (np.sum(preds, axis=0) >= (len(models) / 2)).astype(int)
    return final_preds



import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
def evaluate_ensemble(name, ensemble_models, X_test, y_test, plot=True):
    # Predictions
    y_pred = ensemble_predict(ensemble_models, X_test)

    # Classification report
    print(f"\n=== {name} Ensemble: Classification Report ===")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    print(f"\n=== {name} Ensemble: Confusion Matrix ===")
    print(confusion_matrix(y_test, y_pred))

    # ROC-AUC (use predicted probabilities)
    y_probas = np.mean([m.predict_proba(X_test)[:, 1] for m in ensemble_models], axis=0)
    auc_score = roc_auc_score(y_test, y_probas)
    print(f"\n{name} Ensemble: ROC-AUC = {auc_score:.4f}")

    # Plot ROC curve
    if plot:
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.figure(figsize=(7,5))
        plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color="blue")
        plt.plot([0, 1], [0, 1], "k--", label="Chance")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve - {name} Ensemble")
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()



# ----------------------------
# Train-Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# Random Forest Ensemble
# ----------------------------
rf_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=RandomForestClassifier()
)
evaluate_ensemble("Random Forest", rf_models, X_test, y_test)


# ----------------------------
# Logistic Regression Ensemble
# ----------------------------
lr_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=LogisticRegression(max_iter=1000)
)
evaluate_ensemble("Logistic Regression", lr_models, X_test, y_test)


# ----------------------------
# XGBoost Ensemble
# ----------------------------
xgb_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=XGBClassifier(
        eval_metric="logloss"
    )
)
evaluate_ensemble("XGBoost", xgb_models, X_test, y_test)





import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target column
target = "GLOF"   # change if yours is "Activity" or another name

# Drop unwanted + target
exclude_cols = ["Year_final", "Latitude", "Longitude","Elevation_m", target]
X = df.drop(columns=exclude_cols)
y = df[target]

# Separate categorical & numeric columns
categorical_cols = ["Lake_type_simplified"]
numeric_cols = [col for col in X.columns if col not in categorical_cols]

# Keep only complete rows (CCA subset)
X_cca = X.dropna()

print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%)
# ==============================================
X_masked = X_cca.copy()
rng = np.random.default_rng(42)

# Mask numeric columns
for col in numeric_cols:
    mask = rng.uniform(size=X_cca.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# Mask categorical column
for col in categorical_cols:
    mask = rng.uniform(size=X_cca.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# ==============================================
# 3. Define preprocessors
# ==============================================
numeric_transformer = Pipeline(steps=[
    ("imputer", IterativeImputer(random_state=42, max_iter=10))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols)
    ]
)

# ==============================================
# 4. Fit & Transform (Impute)
# ==============================================
X_imputed = preprocessor.fit_transform(X_masked)

# Rebuild imputed dataframe
imputed_feature_names = (
    numeric_cols +
    list(preprocessor.named_transformers_["cat"]["onehot"].get_feature_names_out(categorical_cols))
)
X_imputed = pd.DataFrame(X_imputed, columns=imputed_feature_names, index=X_masked.index)

# ==============================================
# 5. Evaluate Imputation Accuracy
# ==============================================
print("\n=== Numeric RMSE ===")
for col in numeric_cols:
    missing_mask = X_masked[col].isna()
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col]
        imputed_vals = X_imputed.loc[missing_mask, col]
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy ===")
for col in categorical_cols:
    missing_mask = X_masked[col].isna()
    if missing_mask.any():
        # Get the imputed column set for this categorical
        onehot_cols = [c for c in X_imputed.columns if c.startswith(col + "_")]
        imputed_cat = X_imputed.loc[missing_mask, onehot_cols].idxmax(axis=1).str.replace(col + "_", "")
        true_cat = X_cca.loc[missing_mask, col].astype(str)
        acc = accuracy_score(true_cat, imputed_cat)
        print(f"{col}: Accuracy = {acc:.3f}")







pip install miceforest



import pandas as pd
import numpy as np
import miceforest as mf
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only
X = df.drop(columns=exclude_cols)

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

# Keep only complete rows (CCA subset) + reset index
X_cca = X.dropna().reset_index(drop=True)
print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%)
# ==============================================
X_masked = X_cca.copy()
rng = np.random.default_rng(42)

for col in X_masked.columns:
    mask = rng.uniform(size=X_masked.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# ==============================================
# 3. Run miceforest Imputation
# ==============================================
kernel = mf.ImputationKernel(
    X_masked,
    save_all_iterations_data=True,
    random_state=42
)

# Run MICE: 5 iterations, 1 dataset
kernel.mice(10, n_datasets=5)

# Get imputed dataset from dataset=0, last iteration
X_imputed = kernel.complete_data(
    dataset=0,
    iteration=kernel.iteration_count() - 1
)

# ==============================================
# 4. Evaluate Imputation Accuracy
# ==============================================
print("\n=== Numeric RMSE ===")
for col in X_cca.select_dtypes(include=[np.number]).columns:
    missing_mask = X_masked[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].values
        imputed_vals = X_imputed.loc[missing_mask, col].values
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy ===")
for col in X_cca.select_dtypes(include=["category"]).columns:
    missing_mask = X_masked[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].astype(str).values
        imputed_vals = X_imputed.loc[missing_mask, col].astype(str).values
        acc = accuracy_score(true_vals, imputed_vals)
        print(f"{col}: Accuracy = {acc:.3f}")




# Basic statistics for each numeric column
desc = X_cca.describe().T  # mean, std, min, max, quartiles
# Add extra columns for range and coefficient of variation
desc["range"] = desc["max"] - desc["min"]
desc["cv"] = desc["std"] / desc["mean"]  # coefficient of variation
print(desc[["mean", "std", "min", "25%", "50%", "75%", "max", "range", "cv"]])




