import pandas as pd
import numpy as np





# --- Step 1: Load the base dataframe ---
df_pos = pd.read_csv("df_pos_calculatedarea.csv")

# Drop unwanted columns
cols_to_drop = ["Lake_type", "id", "Lake_area_ha", "Unnamed: 0"]
df_pos = df_pos.drop(columns=cols_to_drop)

# --- Step 2: Load expansion rates ---
exp5 = pd.read_csv("pos_expansion5y.csv")
exp10 = pd.read_csv("pos_expansion10y.csv")

# Keep only 'expansion_ha_peryr' from each and rename
exp5 = exp5[["expansion_ha_peryr"]].rename(columns={"expansion_ha_peryr": "5y_expansion_rate"})
exp10 = exp10[["expansion_ha_peryr"]].rename(columns={"expansion_ha_peryr": "10y_expansion_rate"})

# Concatenate them horizontally
df_pos = pd.concat([df_pos, exp5, exp10], axis=1)

# --- Step 3: Load glacier features ---
glac = pd.read_csv("glac_pos_list_correct_slope.csv")

# Keep only the required glacier columns
needed_glacier_cols = [
    "glacier_area_ha",
    "slope_glac_to_lake",
    "glacier_contact",
    "glacier_touch_count",
    "nearest_glacier_dist_m",
    "glacier_elev_m"
]
glac = glac[needed_glacier_cols]

# Concatenate glacier features to main df
df_pos = pd.concat([df_pos, glac], axis=1)


# --- Step 5: Rename final dataframe ---
uncleaned_ml_pos = df_pos

# --- Final check ---
print("Final Positive dataset shape:", uncleaned_ml_pos.shape)
uncleaned_ml_pos.head()




#convert to CSV file
uncleaned_ml_pos.to_csv("uncleaned_ml_pos.csv",index=False)





import pandas as pd

# --- Step 1: Load the negative base list ---
neg = pd.read_csv("glac_neg_list_correct_slope.csv")

# Drop unwanted columns
neg = neg.drop(columns=["Unnamed: 0", "Lake_type", "Lake_area_ha", "area_t1", "area_t2", "expansion_ha_peryr"], errors="ignore")

# --- Step 2: Load expansion rates ---
exp5 = pd.read_csv("neg_expansion5y.csv")[["expansion_ha_peryr"]] \
          .rename(columns={"expansion_ha_peryr": "5y_expansion_rate"})
exp10 = pd.read_csv("neg_expansion10y.csv")[["expansion_ha_peryr"]] \
           .rename(columns={"expansion_ha_peryr": "10y_expansion_rate"})

# --- Step 3: Concatenate expansions with base ---
uncleaned_ml_neg = pd.concat(
    [neg.reset_index(drop=True),
     exp5.reset_index(drop=True),
     exp10.reset_index(drop=True)],
    axis=1
)

# --- Final check ---
print("Final NEGATIVE dataset shape:", uncleaned_ml_neg.shape)
uncleaned_ml_neg.head()



#convert to CSV file
uncleaned_ml_neg.to_csv("uncleaned_ml_neg.csv",index=False)


# --- Define the correct final column order ---
final_order = [
    "Longitude",
    "Latitude",
    "Year_final",
    "Lake_area_calculated_ha",
    "Elevation_m",
    "Lake_type_simplified",
    "is_supraglacial",
    "glacier_area_ha",
    "slope_glac_to_lake",
    "glacier_contact",
    "glacier_touch_count",
    "nearest_glacier_dist_m",
    "glacier_elev_m",
    "5y_expansion_rate",
    "10y_expansion_rate",
    "GLOF"
]




# --- Reorder negative dataframe ---
uncleaned_ml_neg = uncleaned_ml_neg[final_order]

# --- Reorder positive dataframe ---
uncleaned_ml_pos = uncleaned_ml_pos[final_order]


uncleaned_ml_pos.head()


uncleaned_ml_neg.head()


# --- Step 2: Enforce correct dtypes ---
dtype_map = {
    # numeric (floats)
    "Longitude": "float",
    "Latitude": "float",
    "Lake_area_calculated_ha": "float",
    "glacier_area_ha": "float",
    "slope_glac_to_lake": "float",
    "nearest_glacier_dist_m": "float",
    "glacier_elev_m": "float",
    "5y_expansion_rate": "float",
    "10y_expansion_rate": "float",

    # integers (nullable)
    "Year_final": "Int64",
    "Elevation_m": "Int64",
    "glacier_touch_count": "Int64",
    #flags
    "is_supraglacial": "Int64",
    "glacier_contact": "Int64",

    # categorical
    "Lake_type_simplified": "category",

    # label
    "GLOF": "Int64"
}

uncleaned_ml_pos = uncleaned_ml_pos.astype(dtype_map)
uncleaned_ml_neg = uncleaned_ml_neg.astype(dtype_map)





uncleaned_ml_pos[:10]


uncleaned_ml_neg[:10]


uncleaned_ml_pos.info()


uncleaned_ml_pos["Lake_type_simplified"].unique()


uncleaned_ml_pos.to_csv("uncleaned_ml_pos.csv")
uncleaned_ml_neg.to_csv("uncleaned_ml_neg.csv")


print("Rows per source:")
print("df_pos_calculatedarea.csv   :", len(pd.read_csv("df_pos_calculatedarea.csv")))
print("pos_expansion5y.csv         :", len(pd.read_csv("pos_expansion5y.csv")))
print("pos_expansion10y.csv        :", len(pd.read_csv("pos_expansion10y.csv")))
print("glac_pos_list_correct_slope :", len(pd.read_csv("glac_pos_list_correct_slope.csv")))



print("Rows per NEGATIVE source:")
print("glac_neg_list_correct_slope.csv :", len(pd.read_csv("glac_neg_list_correct_slope.csv")))
print("neg_expansion5y.csv             :", len(pd.read_csv("neg_expansion5y.csv")))
print("neg_expansion10y.csv            :", len(pd.read_csv("neg_expansion10y.csv")))







#convert to CSV file
uncleaned_ml_pos.to_csv("uncleaned_ml_pos.csv",index=False)
uncleaned_ml_neg.to_csv("uncleaned_ml_neg.csv",index=False)





# Load both datasets
pos = pd.read_csv("uncleaned_ml_pos.csv")
neg = pd.read_csv("uncleaned_ml_neg.csv")

# Concatenate with pos first, neg second
uncleaned_ml_combined = pd.concat([pos, neg], axis=0, ignore_index=True)

# Quick check
print("Shape:", uncleaned_ml_combined.shape)
print(uncleaned_ml_combined["GLOF"].value_counts())
uncleaned_ml_combined.head()



#To CSV file:
uncleaned_ml_combined.to_csv("uncleaned_ml_combined.csv",index=False)





uncleaned_ml_pos.shape


# Load (no index read)
uncleaned_ml_pos = pd.read_csv("uncleaned_ml_pos.csv", index_col=False)
df_pos_calculatedarea = pd.read_csv("df_pos_calculatedarea.csv", index_col=False)

# (Already verified) alignment check — optional to keep
assert (uncleaned_ml_pos[['Latitude','Longitude','Year_final']].reset_index(drop=True)
        == df_pos_calculatedarea[['Latitude','Longitude','Year_final']].reset_index(drop=True)).all().all()

# Before counts
before_missing = uncleaned_ml_pos['Lake_area_calculated_ha'].isna().sum()

# Fill-by-index where missing
uncleaned_ml_pos_1 = uncleaned_ml_pos.copy()
uncleaned_ml_pos_1['lake_area_filled_from_db'] = False

mask = uncleaned_ml_pos_1['Lake_area_calculated_ha'].isna() & df_pos_calculatedarea['Lake_area_ha'].notna()
uncleaned_ml_pos_1.loc[mask, 'Lake_area_calculated_ha'] = df_pos_calculatedarea.loc[mask, 'Lake_area_ha']
uncleaned_ml_pos_1.loc[mask, 'lake_area_filled_from_db'] = True

# (Optional) ensure numeric dtype for area
uncleaned_ml_pos_1['Lake_area_calculated_ha'] = pd.to_numeric(uncleaned_ml_pos_1['Lake_area_calculated_ha'], errors='coerce')

# After checks
after_missing = uncleaned_ml_pos_1['Lake_area_calculated_ha'].isna().sum()
print(f"Rows total: {len(uncleaned_ml_pos_1)} (should still be 241)")
print(f"Missing before: {before_missing}  |  filled from DB: {mask.sum()}  |  missing after: {after_missing}")
print("Filled flag True count:", uncleaned_ml_pos_1['lake_area_filled_from_db'].sum())


uncleaned_ml_pos_1.head()


uncleaned_ml_pos_1.isnull().mean()*100


uncleaned_ml_pos_1.to_csv("uncleaned_ml_pos_1.csv")


# Load (no index read)
uncleaned_ml_neg = pd.read_csv("uncleaned_ml_neg.csv", index_col=False)
glac_neg_list_correct_slope = pd.read_csv("glac_neg_list_correct_slope.csv", index_col=False)
# (Already verified) alignment check — optional to keep
assert (uncleaned_ml_neg[['Latitude','Longitude','Year_final']].reset_index(drop=True)
        == glac_neg_list_correct_slope[['Latitude','Longitude','Year_final']].reset_index(drop=True)).all().all()
# Before counts
before_missing = uncleaned_ml_neg['Lake_area_calculated_ha'].isna().sum()
# Fill-by-index where missing
uncleaned_ml_neg_1 = uncleaned_ml_neg.copy()
uncleaned_ml_neg_1['lake_area_filled_from_db'] = False
mask = uncleaned_ml_neg_1['Lake_area_calculated_ha'].isna() & glac_neg_list_correct_slope['Lake_area_ha'].notna()
uncleaned_ml_neg_1.loc[mask, 'Lake_area_calculated_ha'] = glac_neg_list_correct_slope.loc[mask, 'Lake_area_ha']
uncleaned_ml_neg_1.loc[mask, 'lake_area_filled_from_db'] = True
# (Optional) ensure numeric dtype for area
uncleaned_ml_neg_1['Lake_area_calculated_ha'] = pd.to_numeric(uncleaned_ml_neg_1['Lake_area_calculated_ha'], errors='coerce')
# After checks
after_missing = uncleaned_ml_neg_1['Lake_area_calculated_ha'].isna().sum()
print(f"Rows total: {len(uncleaned_ml_neg_1)} (should still be 2411)")
print(f"Missing before: {before_missing}  |  filled from DB: {mask.sum()}  |  missing after: {after_missing}")
print("Filled flag True count:", uncleaned_ml_neg_1['lake_area_filled_from_db'].sum())


uncleaned_ml_neg_1.head()


uncleaned_ml_neg_1.isnull().mean()*100


uncleaned_ml_neg_1.to_csv("uncleaned_ml_neg_1.csv")



