











# Install required libraries
!pip install earthengine-api geemap --quiet






# Import packages
import ee
import geemap
import pandas as pd
import os






df=pd.read_csv('glofdatabase_v4-1.csv')
pd.set_option('display.max_columns',None)
df.shape


df.head(5)








ee.Authenticate()
ee.Initialize(project='fleet-hawk-463015-h5')










# Create interactive map
Map = geemap.Map(basemap='SATELLITE',center=[27.71, 85.32],zoom=7)
Map.add_draw_control()


# Load all countries from Earth Engine
world_borders = ee.FeatureCollection("FAO/GAUL/2015/level1")

styled_world = world_borders.style(**{
    'color': 'red',
    'width': 2,
    'fillColor': '00000000'  # Transparent fill
})
Map.addLayer(styled_world, {}, 'FAO Country Borders')

Map









roi = Map.user_roi
if roi is None:
  print("No ROI selected, Please select roi using the interactive map")
else:
  print("ROI loaded")
  print(type(roi))

#Map.user_roi is a variable that holds an object of class ee.Geometry. it is assigned the geometry object when the user draws on the map



def count_lakes(lakes_fc):
    count = lakes_fc.size()
    n = count.getInfo()
    print(f" Number of lakes detected: {n}")
    return n









import ee


def detect_lake_from_point(point, start_date, end_date,
                        search_radius=800, ndwi_thresh=0.30):

    aoi = point.buffer(search_radius)

    # Load Landsat 8/9 SR
    collection = (ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
                  .filterDate(start_date, end_date)
                  .filterBounds(aoi))

    # Scale factors for SR
    def scale_sr(img):
        optical = img.select('SR_B[0-9]*').multiply(0.0000275).add(-0.2)
        thermal = img.select('ST_B.*').multiply(0.00341802).add(149.0)
        return img.addBands(optical, overwrite=True).addBands(thermal, overwrite=True)

    collection = collection.map(scale_sr)

    # Add NDWI band
    def add_ndwi(img):
        ndwi = img.normalizedDifference(['SR_B3', 'SR_B5']).rename('NDWI')
        return img.addBands(ndwi)
    collection = collection.map(add_ndwi)

    # Check empty collection
    if collection.size().getInfo() == 0:
        raise Exception("No usable Landsat images found")

    # Median NDWI
    ndwi_median = collection.select('NDWI').median().clip(aoi)

    # Safeguard: check band names
    if len(ndwi_median.bandNames().getInfo()) == 0:
        raise Exception("NDWI band missing after processing")

    # Water mask
    water_mask = ndwi_median.gt(ndwi_thresh)

    # Vectorize
    lake_fc_all = water_mask.selfMask().reduceToVectors(
        geometry=aoi,
        scale=30,
        geometryType='polygon',
        reducer=ee.Reducer.countEvery(),
        maxPixels=1e13
    )

    def add_area(f): return f.set('area_km2', f.geometry().area(maxError=1).divide(1e6))
    lake_fc_all = lake_fc_all.map(add_area)

    largest = ee.Feature(lake_fc_all.sort('area_km2', False).first())

    return {
        'lake_fc': ee.FeatureCollection([largest]),
        'area_km2': largest.get('area_km2'),
        'ndwi_median': ndwi_median
    }





lat, lon = 36.879,73.704

point = ee.Geometry.Point([lon, lat])

# Call Landsat-only function
res = detect_lake_from_point(
    point,
    start_date="2016-09-01",
    end_date="2016-12-30"
)

# Print lake area
print("Lake area (km²):", res['area_km2'].getInfo())


'''labels.reduceRegion(...) → Summarizes the values of labels within a specific geometry (here: point).

reducer=ee.Reducer.first() → Returns the first non-null pixel value in that geometry.

Since the geometry is a single point, "first" just means "that pixel value".

geometry=point → The exact point you passed to the function.

scale=10 → Pixel size in meters; matches Sentinel-2 resolution.

.get('labels') → Extracts the numeric label ID from the dictionary returned by reduceRegion.

If the point is on water: returns a positive integer (e.g., 3).

If the point is not on water: returns None (null).'''


lake_point = ee.Geometry.Point([73.704, 36.879])

ndwi_img, lake_poly = detect_lake_from_point(
    point=lake_point,
    start_date='2022-06-01',
    end_date='2022-09-30',
    search_radius=2000,  # try bigger radius
    ndwi_thresh=0.2      # try looser threshold
)

Map = visualize_ndwi_and_lakes(
    ndwi_image=ndwi_img,
    lakes_fc=lake_poly,
    polygon=lake_point.buffer(2000),
    year=2022
)
Map







ndwi_2024, lakes_2024= detect_lakes_by_ndwi(2024,roi)






count_lakes(lakes_2024)


import geemap

def visualize_ndwi_and_lakes(ndwi_image, lakes_fc, polygon, year=2024):
    """
    Visualizes NDWI image and detected lakes on an interactive map.
    """
    Map = geemap.Map(basemap='SATELLITE',center=[36.879, 73.704], zoom=10)
    Map.addLayer(ndwi_image, {'min': 0, 'max': 1, 'palette': ['white', 'blue']}, f'NDWI {year}')
    Map.addLayer(lakes_fc.style(**{'color': 'cyan', 'fillColor': '0000ff80'}), {}, f'Lakes {year}')
    Map.addLayer(polygon, {'color': 'red'}, 'ROI')

    return Map


visualize_ndwi_and_lakes(ndwi_2024, lakes_2024, roi)





















# Add centroid coordinates to each feature
def add_centroid_coords(feature):

    # Get the centroid of the feature's geometry (lake polygon)
    #   - ee.ErrorMargin(1) specifies a 1-meter error tolerance in calculation.
    #   - The result is an ee.Geometry.Point object.
    centroid = feature.geometry().centroid(ee.ErrorMargin(1))

    # Add two new properties to the feature:
    #   'centroid_lat' → the latitude of the centroid (index 1 in [lon, lat])
    #   'centroid_lon' → the longitude of the centroid (index 0 in [lon, lat])
    #   centroid.coordinates() returns an ee.List: [longitude, latitude]
    return feature.set({
        'centroid_lat': centroid.coordinates().get(1),  # latitude
        'centroid_lon': centroid.coordinates().get(0)   # longitude
    })




# Add a unique lake ID to each feature
def add_lake_id(feature):
    """
    Creates a reproducible unique lake_id from centroid coordinates.
    This helps track the same lake across years or datasets.
    """

    # Get centroid latitude from feature properties, ensure it's an ee.Number,
    # and format it to a string with exactly 6 decimal places for consistency.
    lat = ee.Number(feature.get('centroid_lat')).format('%.6f')

    # Do the same for centroid longitude
    lon = ee.Number(feature.get('centroid_lon')).format('%.6f')

    # Build the lake_id string in the format: lake_<lat>_<lon>
    #   - ee.String('lake_') → start with the prefix
    #   - .cat(lat) → append latitude string
    #   - .cat('_') → add an underscore separator
    #   - .cat(lon) → append longitude string
    lake_id_str = ee.String('lake_').cat(lat).cat('_').cat(lon)

    # Add the new 'lake_id' property to the feature's property dictionary
    # and return the updated feature
    return feature.set('lake_id', lake_id_str)


# Add area for a given year
def add_lake_area(feature, year, scale=30):
    """Calculates lake area (km²) and stores it as a property."""

    # Create an image where each pixel value = pixel area in km²
    pixel_area_km2 = ee.Image.pixelArea().divide(1e6).rename('area')

    # Sum pixel areas inside the lake polygon
    area = pixel_area_km2.reduceRegion(
        reducer=ee.Reducer.sum(),        # sum all pixel values
        geometry=feature.geometry(),     # use lake's polygon
        scale=scale,                     # resolution in meters
        maxPixels=1e13                    # allow large calculations
    ).get('area')

    # Add computed area to feature properties
    return feature.set(f'area_km2_{year}', area)


#Enrich lakes with available properties
#1
def enrich_lakes(lakes_fc, year, scale=30):
    """Adds centroid, lake ID, and area to each feature."""
    lakes_fc = lakes_fc.map(add_centroid_coords)
    lakes_fc = lakes_fc.map(add_lake_id)
    lakes_fc = lakes_fc.map(lambda f: add_lake_area(f, year, scale))
    return lakes_fc


#Convert FeatureCollection to Pandas DataFrame
def lakes_to_df(lakes_fc):
    """Converts EE FeatureCollection to a Pandas DataFrame."""
    return geemap.ee_to_df(lakes_fc)


lakes_fc_2024 = enrich_lakes(lakes_2024, 2024, 30)
df_2024 = lakes_to_df(lakes_fc_2024)


df_2024.head()


# 1. Function to add elevation to each feature
def add_lake_elevation(feature):
    dem = ee.Image("USGS/SRTMGL1_003")  # SRTM 30m DEM
    elevation = dem.reduceRegion(
        reducer=ee.Reducer.first(),
        geometry=feature.geometry().centroid(1),
        scale=30
    ).get('elevation')
    return feature.set('elevation_m', elevation)

# 2. Update enrich_lakes to include elevation
def enrich_lakes(lakes_fc, year, scale=30):
    lakes_fc = lakes_fc.map(add_centroid_coords)
    lakes_fc = lakes_fc.map(add_lake_id)
    lakes_fc = lakes_fc.map(lambda f: add_lake_area(f, year, scale))
    lakes_fc = lakes_fc.map(add_lake_elevation)
    return lakes_fc

lakes_fc_2024 = enrich_lakes(lakes_2024, 2024, 30)
df_2024 = lakes_to_df(lakes_fc_2024)


df_2024.head()






















































import ee
import pandas as pd
ee.Initialize()

def get_landsat_collection(year):
    if year <= 2011:
        return 'LANDSAT/LT05/C02/T1_L2'
    elif year < 2013:
        return 'LANDSAT/LE07/C02/T1_L2'
    else:
        return 'LANDSAT/LC08/C02/T1_L2'

def get_landsat_ndwi_area(lat, lon, year, buffer_m=60):
    geom = ee.Geometry.Point(lon, lat).buffer(buffer_m)
    collection_id = get_landsat_collection(year)
    col = (ee.ImageCollection(collection_id)
           .filterDate(f"{year}-06-01", f"{year}-09-30")
           .filterBounds(geom))
    
    # Apply cloud mask & NDWI
    def add_ndwi(img):
        band_green, band_nir = ('SR_B2', 'SR_B4') if 'LT05' in collection_id or 'LE07' in collection_id else ('SR_B3', 'SR_B5')
        ndwi = img.normalizedDifference([band_green, band_nir]).rename('NDWI')
        return img.addBands(ndwi)
    
    def mask_clouds(img):
        qa = img.select('QA_PIXEL')
        cloud_mask = qa.bitwiseAnd(1 << 3).eq(0) \
                   .And(qa.bitwiseAnd(1 << 4).eq(0))
        return img.updateMask(cloud_mask)
    
    ndwi_median = (col.map(mask_clouds)
                      .map(add_ndwi)
                      .median())
    
    water_mask = ndwi_median.select('NDWI').gt(0.3)
    area_m2 = water_mask.multiply(ee.Image.pixelArea()) \
                        .reduceRegion(ee.Reducer.sum(), geom, 30) \
                        .get('NDWI')
    
    return ee.Number(area_m2).divide(1e6)  # km²

def calculate_expansion_rate(lat, lon, year_start, year_end):
    area_start = get_landsat_ndwi_area(lat, lon, year_start).getInfo()
    area_end = get_landsat_ndwi_area(lat, lon, year_end).getInfo()
    rate = (area_end - area_start) / (year_end - year_start)
    return area_start, area_end, rate

def apply_expansion_rate_to_df(df):
    results = []
    for _, row in df.iterrows():
        if pd.notna(row.get('glof_year')):
            year_end = int(row['glof_year']) - 1
            year_start = year_end - 10
        else:
            year_end = 2024
            year_start = 2014
        a_start, a_end, rate = calculate_expansion_rate(row['latitude'], row['longitude'], year_start, year_end)
        results.append({'area_start': a_start, 'area_end': a_end, 'expansion_rate': rate})
    return pd.concat([df.reset_index(drop=True), pd.DataFrame(results)], axis=1)















import ee
ee.Initialize()
# Load SRTM DEM (30m resolution)
dem = ee.Image("USGS/SRTMGL1_003")

def get_elevation(lat, lon):
    """Return elevation (m) from SRTM DEM for a single lat/lon."""
    point = ee.Geometry.Point([lon, lat])
    elev = dem.sample(point, scale=30).first().get("elevation")
    return elev.getInfo()

# Example test
print(get_elevation(27.98, 86.92))   # Should return ~elevation in meters



# Apply row-wise
df_gloflakes["Elevation"] = df_gloflakes.apply(
    lambda row: get_elevation(row["Latitude"], row["Longitude"]), axis=1
)












import pandas as pd

# Show all rows
pd.set_option("display.max_rows", None)
# Show all columns
pd.set_option("display.max_columns", None)

# Load only High Mountain Asia sheet
df_gloflakes = pd.read_excel(
    "glofdatabase_v4-1.xlsx",
    sheet_name="High Mountain Asia",
    header=0,
    skiprows=[1, 2]   # skip metadata + example rows
)


df_gloflakes.head(1)
print(df_gloflakes.columns.tolist())


print(df_gloflakes["Date"].dtype)



import pandas as pd
# Remove leading/trailing spaces in column names
df_gloflakes.columns = df_gloflakes.columns.str.strip()

# --- Parse dates ---
df_gloflakes["Year"] = pd.to_datetime(df_gloflakes["Date"], errors="coerce").dt.year
df_gloflakes["Year_min"] = pd.to_datetime(df_gloflakes["Date_Min"], errors="coerce").dt.year
df_gloflakes["Year_max"] = pd.to_datetime(df_gloflakes["Date_Max"], errors="coerce").dt.year

# --- Final year: prefer reported, else Year_min, else NaN ---
df_gloflakes["Year_final"] = (
    df_gloflakes["Year"]
    .fillna(df_gloflakes["Year_min"])
    .fillna(df_gloflakes["Year_max"])
)
df_gloflakes["Year_final"] = df_gloflakes["Year_final"].astype("Int64")


df_gloflakes.head(5)


# Strip whitespace and keep only 4-digit year patterns
import re

def extract_year(val):
    if pd.isna(val):
        return None
    m = re.search(r"\b\d{4}\b", str(val))  # grab 4-digit year
    return int(m.group(0)) if m else None

df_gloflakes["Year"] = df_gloflakes["Date"].apply(extract_year)
df_gloflakes["Year_min"] = df_gloflakes["Date_Min"].apply(extract_year)
df_gloflakes["Year_max"] = df_gloflakes["Date_Max"].apply(extract_year)

df_gloflakes["Year_final"] = (
    df_gloflakes["Year"]
    .fillna(df_gloflakes["Year_min"])
    .fillna(df_gloflakes["Year_max"])
).astype("Int64")
df_gloflakes.head(5)


#convert lake area from square meters to hectares
# Ensure df_gloflakes is a copy
df_gloflakes = df_gloflakes.copy()

# Convert to numeric (m²)
df_gloflakes["Lake_area_before"] = pd.to_numeric(
    df_gloflakes["Lake_area_before"], errors="coerce"
)

# Convert m² → hectares
df_gloflakes["Lake_area_before_ha"] = df_gloflakes["Lake_area_before"] * 0.0001

# Drop old column
df_gloflakes = df_gloflakes.drop(columns=["Lake_area_before"])

df_gloflakes.head(4)


keep_cols = ["Lake_type",
    "Longitude", "Latitude",
    "Year_final",
    "Lake_area_before_ha"
]
df_gloflakes = df_gloflakes[keep_cols]
df_gloflakes.head(5)


df_gloflakes = df_gloflakes.rename(columns={
    "Lake_area_before_ha": "Lake_area(ha)"
})
df_gloflakes.head(5)





# --- Filter only lakes after 1984 ---
df_gloflakes = df_gloflakes[df_gloflakes["Year_final"] >= 1984]

#--drop lakes with no longitude or latitude
df_gloflakes = df_gloflakes.dropna(subset=["Longitude", "Latitude"])
df_gloflakes = df_gloflakes.reset_index(drop=True)

df_gloflakes.head(10)


df_gloflakes["Latitude"] = df_gloflakes["Latitude"].round(3)
df_gloflakes["Longitude"] = df_gloflakes["Longitude"].round(3)

df_gloflakes.head(5)


# Get elevation:
df_gloflakes["Elevation(m)"] = df_gloflakes.apply(
    lambda row: get_elevation(row["Latitude"], row["Longitude"]), axis=1
)



df_gloflakes








import pandas
df_lakes= pd.read_excel("IHR_final_list1.xlsx")
df_lakes.head(5)
# Round to 3 decimals (~100m precision)

df_lakes["Latitude"] = df_lakes["Latitude"].round(3)
df_lakes["Longitude"] = df_lakes["Longitude"].round(3)
df_lakes["Year_final"] = 2019

df_lakes.head(10)


df_lakes = df_lakes.rename(columns={
    "GL type": "Lake_type",
    "Elev(m)": "Elevation(m)",
    "Area(ha)": "Lake_area(ha)"
})
df_lakes.head(2)





keep_cols2 = ["Lake_type",
    "Lake_area(ha)", "Elevation(m)",
    "Longitude", "Latitude",
    "Year_final"
]

df_lakes = df_lakes[keep_cols2]
df_lakes.head(5)





!pip install scikit-learn --quiet


from sklearn.neighbors import BallTree
import numpy as np

# Convert coords to radians
rad = np.pi / 180
ind_rad = np.c_[df_lakes["Latitude"]*rad, df_lakes["Longitude"]*rad]
glof_rad = np.c_[df_gloflakes["Latitude"]*rad, df_gloflakes["Longitude"]*rad]

# Build BallTree on Indian lakes
tree = BallTree(ind_rad, metric="haversine")

# Query nearest Indian lake for each GLOF lake
dist, idx = tree.query(glof_rad, k=1)  # k=1 → nearest neighbor
dist_m = dist.flatten() * 6371000  # convert radians to meters



df_gloflakes["nearest_ind_idx"] = idx.flatten()
df_gloflakes["dist_to_indian_m"] = dist_m



# Consider it a match if within 1000 m
df_gloflakes["matched"] = df_gloflakes["dist_to_indian_m"] <= 1000



# Default all Indian lakes = 0
df_lakes["GLOF"] = 0

# Find all Indian lake indices that got matched
matched_idx = df_gloflakes.loc[df_gloflakes["matched"], "nearest_ind_idx"].unique()

# Mark those Indian lakes as positive
df_lakes.loc[matched_idx, "GLOF"] = 1

# Count positives vs negatives
print(df_lakes["GLOF"].value_counts())
df_lakes[df_lakes["GLOF"] == 1]



print(df_gloflakes.loc[df_gloflakes["matched"], "dist_to_indian_m"].describe())



# Keep only negatives (GLOF = 0)
df_lakes_neg = df_lakes[df_lakes["GLOF"] == 0].copy()

print("Original lakes:", len(df_lakes))
print("After dropping matched:", len(df_lakes_neg))




df_gloflakes["GLOF"] = 1

# Combine into one dataset
df_final = pd.concat([df_gloflakes, df_lakes_neg], ignore_index=True)

print(df_final["GLOF"].value_counts())



df_final[230:260]


keep_cols3 = [
    "Lake_type",
    "Longitude", "Latitude",
    "Year_final",
    "Lake_area(ha)",
    "Elevation(m)",
    "GLOF"
]
df_final = df_final[keep_cols3]
df_final.describe()
df_final[230:260]


df_final.to_csv("prefeaturelist.csv",index=False)


# Define mapping for simplified categories
lake_type_map = {
    # Moraine group
    "moraine": "moraine",
    "moraine/bedrock": "moraine",
    "M(e)": "moraine",
    "M(l)": "moraine",
    "M(o)": "moraine",
    "M(lg)": "moraine",
    
    # Ice group
    "ice": "ice",
    "supraglacial": "ice",
    "I(s)": "ice",
    "I(d)": "ice",
    
    # Other group
    "bedrock": "other",
    "water pocket": "other",
    "E(c)": "other",
    "E(o)": "other",
    "E(v)": "other",
    "O": "other",
    "unknown": "other"
}

# Apply mapping
df_final["Lake_type_simplified"] = df_final["Lake_type"].map(lake_type_map)

# Add supraglacial flag
df_final["is_supraglacial"] = df_final["Lake_type"].isin(["supraglacial", "I(s)"]).astype(int)

# Quick check
print(df_final[["Lake_type", "Lake_type_simplified", "is_supraglacial"]].head(20))



df_final[200:250]


df_final.to_csv("df_final_with_laketypesorted_pre.csv",index=False)


df_final = df_final.rename(columns={'Lake_area(ha)': "Lake_area_ha",'Elevation(m)':"Elevation_m"})


df_final.to_csv("df_final_with_laketypesorted_pre.csv",index=False)





import pandas as pd

# 1) Load
glac = pd.read_csv("glac_pos_list_correct_slope.csv")
print("Raw shape:", glac.shape)

# 2) Drop duplicates using the saved-index column
#    (keeps the first occurrence of each index value)
glac = glac.drop_duplicates(subset=["Unnamed: 0"], keep="first")
print("After dedup by 'Unnamed: 0':", glac.shape)

# 3) Remove that index column (we don't need it downstream)
glac = glac.drop(columns=["Unnamed: 0"])
print("After dropping 'Unnamed: 0':", glac.shape)

glac.to_csv("glac_pos_list_correct_slope.csv", index=False)















#this code works perfectly. uses 3 years to get composit ndwi and changes the landsat version as required


#------------Final Detect_lake_from_point------------#ddd
import ee
import pandas as pd


def detect_lake_from_point(point, year, search_radius=3000, ndwi_thresh=0.25):
    aoi = point.buffer(search_radius)

    def mask_landsat_clouds(img):
        qa = img.select('QA_PIXEL')
        cloud        = 1 << 3
        cloud_shadow = 1 << 4
        snow         = 1 << 5
        mask = (qa.bitwiseAnd(cloud).eq(0)
                  .And(qa.bitwiseAnd(cloud_shadow).eq(0))
                  .And(qa.bitwiseAnd(snow).eq(0)))
        return img.updateMask(mask)

    def scale_sr(img):
        optical = img.select('SR_B[0-9]*').multiply(0.0000275).add(-0.2)
        return img.addBands(optical, overwrite=True)

    # --- NDWI depending on collection ---
    def add_ndwi(img, sensor):
        if sensor in ["LC08", "LC09"]:   # Landsat 8/9
            green, nir = 'SR_B3', 'SR_B5'
        else:                            # Landsat 5/7
            green, nir = 'SR_B2', 'SR_B4'
        ndwi = img.normalizedDifference([green, nir]).rename('NDWI')
        return img.addBands(ndwi)


    # --- Choose the correct Landsat collection ---
    def get_collection_id(y):
        if y <= 2011:
            return "LANDSAT/LT05/C02/T1_L2"   # Landsat 5
        elif y == 2012:
            return "LANDSAT/LE07/C02/T1_L2"   # Landsat 7
        else:
            return "LANDSAT/LC08/C02/T1_L2"   # Landsat 8/9

    def get_fall_collection(y):
        coll_id = get_collection_id(y)
        sensor = coll_id.split("/")[1][:4]  # e.g. LC08, LT05, LE07
        return (ee.ImageCollection(coll_id)
            .filterDate(f"{y}-08-01", f"{y}-12-31")
            .filterBounds(aoi)
            .map(mask_landsat_clouds)
            .map(scale_sr)
            .map(lambda img: add_ndwi(img, sensor)))

    # --- Combine 3 years ---
    collection = (get_fall_collection(year-1)
                  .merge(get_fall_collection(year))
                  .merge(get_fall_collection(year+1)))

    if collection.size().getInfo() == 0:
        return {'lake_fc': ee.FeatureCollection([]), 'area_ha': None}

    # --- Median composite ---
    ndwi_median = collection.select('NDWI').median().clip(aoi)
    water_mask = ndwi_median.gt(ndwi_thresh)

    lake_fc_all = water_mask.selfMask().reduceToVectors(
        geometry=aoi,
        scale=30,
        geometryType='polygon',
        reducer=ee.Reducer.countEvery(),
        maxPixels=1e13
    )

    def add_area(f):
        return f.set('area_ha', f.geometry().area(maxError=1).divide(1e4))
    lake_fc_all = lake_fc_all.map(add_area)

    largest = ee.Feature(lake_fc_all.sort('area_ha', False).first())

    return {
        'lake_fc': ee.Algorithms.If(
            largest,
            ee.FeatureCollection([largest]),
            ee.FeatureCollection([])
        ),
        'area_ha': ee.Algorithms.If(largest, largest.get('area_ha'), None),
        'ndwi_median': ndwi_median
    }
















# --- Load your CSV ---
df = pd.read_csv("IHR_GlacialLake_Atlas table_cleaned.csv") 

# ---- First 5 rows ----
df_first5 = df.head(30).copy()
areas_first5 = []

for _, row in df_first5.iterrows():
    lat, lon = row["Latitude"], row["Longitude"]
    if pd.isna(lat) or pd.isna(lon):
        areas_first5.append(None)
        continue

    point = ee.Geometry.Point([lon, lat])
    try:
        result = detect_lake_from_point(point, year=2016)
        area = result['area_ha'].getInfo() if result['area_ha'] else None
    except Exception as e:
        print(f"Error at {lat},{lon}: {e}")
        area = None
    areas_first5.append(area)

df_first5["Area_ha by nima"] = areas_first5
df_first5








#Expansion Rate for nonglof lakes


def expansion_rate_nonglof(point, ref_year, search_radius=3000, ndwi_thresh=0.3):
    """
    Calculate 10-year expansion rate of a non-GLOF lake (ha/year).

    - ref_year: e.g., 2020
    - recent window: (ref_year-1, ref_year, ref_year+1)
    - past window:   (ref_year-11, ref_year-10, ref_year-9)
    - search_radius default set to 3000 m
    """
    
    # --- Recent ---
    recent = detect_lake_from_point(point, ref_year,
                                    search_radius=search_radius,
                                    ndwi_thresh=ndwi_thresh)
    area_recent = recent["area_ha"].getInfo() if recent["area_ha"] else None
    
    # --- Past (10 years earlier) ---
    past_year = ref_year - 10
    past = detect_lake_from_point(point, past_year,
                                  search_radius=search_radius,
                                  ndwi_thresh=ndwi_thresh)
    area_past = past["area_ha"].getInfo() if past["area_ha"] else None
    
    # --- Expansion rate ---
    if area_recent is None or area_past is None:
        print(f"[{ref_year}] Could not compute area(s). "
              f"Recent={area_recent}, Past={area_past}")
        return None
    
    rate = (area_recent - area_past) / 10.0   # ha per year
    
    # --- Print for checking ---
    print(f"Ref year {ref_year}: Past={area_past:.2f} ha | "
          f"Recent={area_recent:.2f} ha | "
          f"Expansion={rate:.2f} ha/yr")
    
    return rate, area_recent, area_past



# ---- First 5 rows ----
df_first5 = df.head(15).copy()

expansion_rates = []
areas_recent = []
areas_past = []

for _, row in df_first5.iterrows():
    lat, lon = row["Latitude"], row["Longitude"]

    if pd.isna(lat) or pd.isna(lon):
        expansion_rates.append(None)
        areas_recent.append(None)
        areas_past.append(None)
        continue

    point = ee.Geometry.Point([lon, lat])
    try:
        result = expansion_rate_nonglof(point, ref_year=2016)  
        if result:
            rate, recent, past = result
        else:
            rate, recent, past = None, None, None
    except Exception as e:
        print(f"Error at {lat},{lon}: {e}")
        rate, recent, past = None, None, None

    expansion_rates.append(rate)
    areas_recent.append(recent)
    areas_past.append(past)

# Add new columns
df_first5["ExpansionRate_ha_peryr"] = expansion_rates
df_first5["AreaRecent_ha"] = areas_recent
df_first5["AreaPast_ha"] = areas_past

df_first5





















import ee, geemap

lat,lon=27.8610,86.4756
lake_point = ee.Geometry.Point(lon,lat)

# Detect lake polygon
lake_result = detect_lake_from_point(
    lake_point, year=2016
)
lake_fc = lake_result['lake_fc']

# Nearest glacier
glaciers = ee.FeatureCollection("GLIMS/current")
nearby = glaciers.filterBounds(lake_point.buffer(50000))
with_dist = nearby.map(lambda g: g.set("distance_m", g.geometry().distance(lake_point)))
nearest = with_dist.sort("distance_m").first()
nearest_geom = ee.Feature(nearest).geometry()

# Connection line
line = ee.Geometry.LineString([lake_point.coordinates(), nearest_geom.centroid().coordinates()])

# -------------------------
# DEM overlay (SRTM 30m)
dem = ee.Image("USGS/SRTMGL1_003")

# Visualization parameters
dem_vis = {
    'min': 4000,   # adjust for Himalayas
    'max': 8000,
    'palette': ['blue', 'green', 'yellow', 'orange', 'red', 'white']
}

# -------------------------
# Map
m = geemap.Map(center=[lat, lon], zoom=11)
m.add_basemap("SATELLITE")

# Add DEM as background layer
m.addLayer(dem, dem_vis, "DEM Elevation")

# Add features
m.addLayer(lake_point, {"color": "blue"}, "Lake Point")
m.addLayer(lake_fc, {"color": "cyan"}, "Lake Polygon (NDWI)")
m.addLayer(nearest_geom, {"color": "black"}, "Nearest Glacier")
m.addLayer(line, {"color": "green"}, "Connection Line")

m










def get_nearest_glacier(lat, lon, buffer_km=50):
    lake_point = ee.Geometry.Point(lon, lat)
    glaciers = ee.FeatureCollection("GLIMS/current")
    dem = ee.Image("USGS/SRTMGL1_003")  # SRTM 30m DEM

    # Filter glaciers within buffer
    nearby = glaciers.filterBounds(lake_point.buffer(buffer_km * 1000))

    # Add distance property
    with_dist = nearby.map(lambda g: g.set("dist", g.geometry().distance(lake_point)))

    # Get nearest glacier
    nearest = ee.Feature(with_dist.sort("dist").first())
    if nearest is None:
        return None

    glacier_geom = nearest.geometry()

    nearest_point = lake_point.closestPoint(glacier_geom)
    nearest_coords = nearest_point.getInfo()["coordinates"]

    # Distance (m)
    distance = glacier_geom.distance(lake_point).getInfo()

    # Area (km²)
    area_km2 = glacier_geom.area().divide(1e6).getInfo()

    # Elevations (m)
    lake_elev = dem.sample(lake_point, 30).first().get("elevation").getInfo()
    glac_elev = dem.sample(nearest_point, 30).first().get("elevation").getInfo()

    # Slope glacier→lake (rise/run, dimensionless)
    slope = None
    if distance > 0 and lake_elev is not None and glac_elev is not None:
        slope = (glac_elev - lake_elev) / distance

    return {
        "lake_coords": [lon, lat],
        "nearest_point_coords": nearest_coords,  # [lon, lat]
        "glacier_area_km2": area_km2,
        "distance_m": distance,
        "lake_elev_m": lake_elev,
        "glacier_elev_m": glac_elev,
        "slope_glac_to_lake": slope,
        "glacier_geom": glacier_geom,
    }
















def visualize_glacier(result, zoom=11):
    """
    Visualize the lake, nearest glacier, and connection line.
    Compatible with get_nearest_glacier() that returns coords as lists.
    """

    if result is None:
        print("No glacier found.")
        return None

    # Rebuild geometries from coords
    lake = ee.Geometry.Point(result["lake_coords"])
    nearest_point = ee.Geometry.Point(result["nearest_point_coords"])
    nearest_point_circle = nearest_point.buffer(50)
    glacier = result["glacier_geom"]

    #make lake bigger circle
    lake_circle = lake.buffer(500) 
    # Build line between lake and nearest point
    line = ee.Geometry.LineString([
        result["lake_coords"],
        result["nearest_point_coords"]
    ])

    # Map center (geemap expects lat, lon order)
    center = result["lake_coords"][::-1]
    m = geemap.Map(center=center, zoom=zoom)
    m.add_basemap("SATELLITE")

    # Add layers
    m.addLayer(lake_circle, {"color": "blue"}, "Lake")
    m.addLayer(nearest_point_circle, {"color": "red"}, "Nearest Glacier Point")
    m.addLayer(glacier, {"color": "purple"}, "Nearest Glacier")
    m.addLayer(line, {"color": "yellow", "width":1}, "Connection Line")



    return m






lat,lon=34.316,80.858
res = get_nearest_glacier(lat, lon)

print("Lake elevation (m):", res["lake_elev_m"])
print("Glacier elevation (m):", res["glacier_elev_m"])
print("Distance (m):", res["distance_m"])
print("Slope (rise/run):", res["slope_glac_to_lake"])
print("Glacier area (km²):", res["glacier_area_km2"])






lat, lon = 28.329,85.869
res = get_nearest_glacier(lat, lon)
m = visualize_glacier(res, zoom=12)
m




glaciers = ee.FeatureCollection("GLIMS/current")
print(glaciers.first().propertyNames().getInfo())










#visualize with elevation


def visualize_glacierE(result, zoom=11):
    """
    Visualize the lake, nearest glacier, and connection line.
    Compatible with get_nearest_glacier() that returns coords as lists.
    """

    if result is None:
        print("No glacier found.")
        return None

    # Rebuild geometries from coords
    lake = ee.Geometry.Point(result["lake_coords"])
    nearest_point = ee.Geometry.Point(result["nearest_point_coords"])
    nearest_point_circle = nearest_point.buffer(50)
    glacier = result["glacier_geom"]
    
    #make lake bigger circle
    lake_circle = lake.buffer(500) 
    # Build line between lake and nearest point
    line = ee.Geometry.LineString([
        result["lake_coords"],
        result["nearest_point_coords"]
    ])
    
    # Map center (geemap expects lat, lon order)
    center = result["lake_coords"][::-1]
    m = geemap.Map(center=center, zoom=zoom)
    m.add_basemap("SATELLITE")

    # DEM overlay (SRTM 30m)
    dem = ee.Image("USGS/SRTMGL1_003")
    # Visualization parameters
    dem_vis = {
         'min': 4000,   
         'max': 8000,
         'palette': ['blue', 'green', 'yellow', 'orange', 'red', 'white']
     }
    m.addLayer(dem, dem_vis, "DEM Elevation")
    # Add layers
    m.addLayer(lake_circle, {"color": "black"}, "Lake")
    m.addLayer(nearest_point_circle, {"color": "red"}, "Nearest Glacier Point")
    m.addLayer(glacier, {"color": "purple"}, "Nearest Glacier")
    m.addLayer(line, {"color": "yellow", "width":1}, "Connection Line")



    return m


m = visualize_glacierE(res, zoom=12)
m






def detect_lakes_by_ndwi(year, polygon, ndwi_thresh=0.3, cloud_thresh=60):
    s2 = ee.ImageCollection("COPERNICUS/S2_SR") \
        .filterDate(f'{year}-06-15', f'{year}-10-15') \
        .filterBounds(polygon) \
        .filter(ee.Filter.lt("CLOUDY_PIXEL_PERCENTAGE", cloud_thresh))

    def compute_ndwi(img):
        return img.normalizedDifference(['B3', 'B8']).rename('NDWI') \
            .copyProperties(img, img.propertyNames())

    ndwi_collection = s2.map(compute_ndwi)
    ndwi_median = ndwi_collection.median().clip(polygon)

    water_mask = ndwi_median.gt(ndwi_thresh).focal_max(1).focal_min(1)

    lake_polygons = water_mask.selfMask().reduceToVectors(
        geometry=polygon,
        scale=10,
        geometryType='polygon',
        reducer=ee.Reducer.countEvery(),
        maxPixels=1e13
    )

    # Filter small lakes < 0.15 km²
    lake_polygons = lake_polygons.map(lambda f: f.set('area', f.geometry().area(20)))
    filtered_lakes = lake_polygons.filter(ee.Filter.gt('area', 150000))

    return ndwi_median, filtered_lakes


#gets the nearest glacier polygons to lake "polygon" and if not touch just gets the closest glacier's info:


def get_nearest_glacier(lake_geom, buffer_km=50):
    """
    Given a lake polygon, finds nearest or touching glacier(s).
    Returns consistent features for ML:
      - glacier_contact (bool)
      - nearest_glacier_dist_m
      - lake_elev_m
      - glacier_elev_m (from chosen glacier)
      - slope_glac_to_lake (rise/run, slope=0 if touching)
      - glacier_area_km2 (sum of all touching glaciers, or area of nearest one)
      - glacier_ids (list of GLIMS IDs for touching glaciers, or [nearest_id])
    """
    glaciers = ee.FeatureCollection("GLIMS/current")
    dem = ee.Image("USGS/SRTMGL1_003")

    # Filter glaciers within buffer
    nearby = glaciers.filterBounds(lake_geom.buffer(buffer_km * 1000))

    # Glaciers that touch/intersect the lake
    touching = nearby.filterBounds(lake_geom)

    # --- Case 1: touching glaciers ---
    if touching.size().getInfo() > 0:
        # Add area for each
        def add_area(f):
            return f.set("glacier_area_km2", f.geometry().area(maxError=30).divide(1e6))
        touching = touching.map(add_area)

        # Glacier with largest area (used for slope reference)
        largest_touching = ee.Feature(touching.sort("glacier_area_km2", False).first())

        # Collect all glacier IDs
        glacier_ids = touching.aggregate_array("glac_id").getInfo()

        # Metrics
        combined_area = touching.aggregate_sum("glacier_area_km2").getInfo()
        lake_elev = dem.sample(lake_geom.centroid(maxError=30), 30).first().get("elevation").getInfo()
        glac_elev = dem.sample(largest_touching.geometry().centroid(maxError=30), 30).first().get("elevation").getInfo()

        return {
            "glacier_contact": True,
            "nearest_glacier_dist_m": 0,   # touching → distance = 0
            "lake_elev_m": lake_elev,
            "glacier_elev_m": glac_elev,
            "slope_glac_to_lake": 0,      # standardized → slope=0 when touching
            "glacier_area_km2": combined_area,   # sum of all touching glaciers
            "glacier_ids": glacier_ids    # all touching glaciers
        }

    # --- Case 2: no touching glaciers ---
    else:
        # Compute distances
        with_dist = nearby.map(lambda g: g.set("dist", g.geometry().distance(lake_geom, maxError=30)))
        nearest = ee.Feature(with_dist.sort("dist").first())

        # Metrics
        distance = nearest.get("dist").getInfo()
        area_km2 = nearest.geometry().area(maxError=30).divide(1e6).getInfo()
        lake_elev = dem.sample(lake_geom.centroid(maxError=30), 30).first().get("elevation").getInfo()
        glac_elev = dem.sample(nearest.geometry().centroid(maxError=30), 30).first().get("elevation").getInfo()

        slope = (glac_elev - lake_elev) / distance if (lake_elev and glac_elev and distance > 0) else None

        return {
            "glacier_contact": False,
            "nearest_glacier_dist_m": distance,
            "lake_elev_m": lake_elev,
            "glacier_elev_m": glac_elev,
            "slope_glac_to_lake": slope,
            "glacier_area_km2": area_km2,
            "glacier_ids": [nearest.get("glac_id").getInfo()]  # single nearest glacier
        }





lat,lon=36.145,73.639
point = ee.Geometry.Point([lon, lat])   # Earth Engine Point, order = [lon, lat]

lake_result = detect_lake_from_point(point, 2016)


# This is a FeatureCollection

lake_fc = ee.FeatureCollection(lake_result["lake_fc"])
lake_geom = ee.Feature(lake_fc.first()).geometry()



print("Type of lake_fc:", type(lake_fc))    # should be <class 'ee.featurecollection.FeatureCollection'>
print("Lake FC size:", lake_fc.size().getInfo())  # number of polygons


m = visualize_glacierE(res, zoom=12)
m


# --- Test one lake ---
lat, lon = 27.864,87.837   # example coords
point = ee.Geometry.Point([lon, lat])

# 1. Detect lake polygon
lake_result = detect_lake_from_point(point, 2016)
lake_fc = ee.FeatureCollection(lake_result["lake_fc"])
print("Lake polygons detected:", lake_fc.size().getInfo())

if lake_fc.size().getInfo() > 0:
    lake_geom = ee.Feature(lake_fc.first()).geometry(maxError=30)
    print("Lake area (km²):", lake_geom.area(maxError=30).divide(1e6).getInfo())

    # 2. Run glacier analysis
    glacier_info = get_nearest_glacier(lake_geom, buffer_km=50)


else:
    print("No lake detected for this point/year.")





# --- Test one lake ---
lat, lon = 27.864,87.837   # example coords
point = ee.Geometry.Point([lon, lat])

# 1. Detect lake polygon
lake_result = detect_lake_from_point(point, 2016)
lake_fc = ee.FeatureCollection(lake_result["lake_fc"])
print("Lake polygons detected:", lake_fc.size().getInfo())
# Only print the summary keys we care about
summary_keys = [
    "touching",
    "distance_m",
    "lake_elev_m",
    "glacier_elev_m",
    "slope_glac_to_lake",
    "glacier_area_km2"
]

for k in summary_keys:
    print(f"{k:20}: {glacier_info.get(k)}")




def visualize_glacierE(lake_geom, glacier_info, zoom=11):
    """
    Visualize the lake polygon, glaciers (touching or nearest), and connection line.
    Compatible with the new get_nearest_glacier() output.
    """
    # Base map
    center = lake_geom.centroid(maxError=30).coordinates().getInfo()[::-1]  # [lat, lon]
    m = geemap.Map(center=center, zoom=zoom)
    m.add_basemap("SATELLITE")

    # DEM overlay (SRTM 30m)
    dem = ee.Image("USGS/SRTMGL1_003")
    dem_vis = {
        "min": 4000,
        "max": 6000,
        "palette": ["blue", "green", "yellow", "orange", "red", "white"]
    }
    m.addLayer(dem, dem_vis, "DEM Elevation")

    # Lake polygon
    m.addLayer(lake_geom, {"color": "cyan"}, "Lake Polygon")

    if glacier_info["touching"]:
        # --- Case 1: Touching glaciers ---
        largest_overlap = ee.Feature(glacier_info["largest_overlap_glacier"])
        m.addLayer(largest_overlap.geometry(), {"color": "purple"}, "Largest Overlap Glacier")

    else:
        # --- Case 2: Nearest glacier ---
        nearest = ee.Feature(glacier_info["nearest_glacier"])
        nearest_point = nearest.geometry().centroid(maxError=30)
        lake_centroid = lake_geom.centroid(maxError=30)

        # Buffers
        m.addLayer(lake_centroid.buffer(200), {"color": "black"}, "Lake Centroid")
        m.addLayer(nearest_point.buffer(100), {"color": "red"}, "Nearest Glacier Point")

        # Glacier polygon
        m.addLayer(nearest.geometry(), {"color": "purple"}, "Nearest Glacier")

        # Connection line
        line = ee.Geometry.LineString([
            lake_centroid.coordinates(),
            nearest_point.coordinates()
        ])
        m.addLayer(line, {"color": "yellow", "width": 2}, "Connection Line")

    return m






m = visualize_glacierE(lake_geom, glacier_info, zoom=11)
m










