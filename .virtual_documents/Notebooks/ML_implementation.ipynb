import pandas as pd
import numpy as np



pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
uncleaned_ml_combined=pd.read_csv("../CSVs/uncleaned_ml_combined.csv",index_col=False)


uncleaned_ml_combined.head()



uncleaned_ml_combined.info()


uncleaned_ml_neg=pd.read_csv("../CSVs/uncleaned_ml_neg.csv")
uncleaned_ml_neg.info()


uncleaned_ml_neg_1=pd.read_csv("../CSVs/uncleaned_ml_neg_1.csv")
uncleaned_ml_neg_1.info()


uncleaned_ml_pos=pd.read_csv("../CSVs/uncleaned_ml_pos.csv")





ml_neg_y_drop=uncleaned_ml_neg.drop(columns=['Year_final'])
ml_neg_y_drop


ml_pos_y_drop=uncleaned_ml_pos.drop(columns=['Year_final'])
ml_pos_y_drop


uncleaned_ml_combined=pd.read_csv('../CSVs/uncleaned_ml_combined.csv')
uncleaned_ml_combined.info()


ml_comb_y_drop=uncleaned_ml_combined.drop(columns=['Year_final'])
ml_comb_y_drop.head()


ml_neg_y_drop.to_csv("../CSVs/ml_neg_y_drop.csv",index=False)
ml_pos_y_drop.to_csv("../CSVs/ml_pos_y_drop.csv",index=False)
ml_comb_y_drop.to_csv("../CSVs/ml_comb_y_drop.csv",index=False)


ml_pos_y_drop.isnull().mean()*100


ml_neg_y_drop.isnull().mean()*100


ml_comb_y_drop.isnull().mean()*100


ml_pos_y_drop.head()





# Load Data
df_comb = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")
df_pos = pd.read_csv("../CSVs/uncleaned_ml_pos.csv")
# Complete Case Analysis (CCA)
df_comb = df_comb.dropna()


print(df_comb.shape)
print(df_pos.shape)


df_pos.head()


# Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.base import clone



# Load Data
df_comb = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Complete Case Analysis (CCA)
df_comb = df_comb.dropna()

# Features and Target
X = df_comb.drop("GLOF", axis=1)
y = df_comb["GLOF"]

# Preprocessing
# Only one-hot encode Lake_type_simplified,
# all others are numeric (including 0/1 binary flags)
cat_features = ["Lake_type_simplified"]
exclude_cols = cat_features + ["Year_final","Latitude", "Longitude"]
num_features = [col for col in X.columns if col not in exclude_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features)
    ]
)

# Ensemble Creation Function
# Undersample majority class into many subsets, train separate classifiers

def create_ensemble(X_train, y_train, n_models=5, base_model=None):
    """
    Create an ensemble of classifiers with undersampling.
    
    Parameters:
    - X_train, y_train: training data
    - n_models: number of models in ensemble
    - base_model: the classifier to use (e.g., RandomForestClassifier(), LogisticRegression(), XGBClassifier())
    """
    if base_model is None:
        base_model = RandomForestClassifier()  # default

    models = []

    # Split into minority (1) and majority (0)
    X_minority = X_train[y_train == 1]
    y_minority = y_train[y_train == 1]
    X_majority = X_train[y_train == 0]
    y_majority = y_train[y_train == 0]

    for i in range(n_models):
        # Undersample majority class
        X_majority_resampled, y_majority_resampled = resample(
            X_majority, y_majority,
            replace=False,
            n_samples=len(y_minority),
            random_state=i
        )

        # Combine resampled majority + minority
        X_balanced = pd.concat([X_majority_resampled, X_minority])
        y_balanced = pd.concat([y_majority_resampled, y_minority])

        # Clone base_model so each one is fresh
        clf = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", clone(base_model).set_params(random_state=i))
            if hasattr(base_model, "random_state") else
            ("classifier", clone(base_model))
        ])

        # Train
        clf.fit(X_balanced, y_balanced)
        models.append(clf)

    return models

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Helper: Ensemble Prediction
def ensemble_predict(models, X):
    preds = np.array([m.predict(X) for m in models])
    # Majority vote: if at least half the models predict 1, label as 1 (else 0)
    final_preds = (np.sum(preds, axis=0) >= (len(models) / 2)).astype(int)
    return final_preds



import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
def evaluate_ensemble(name, ensemble_models, X_test, y_test, plot=True):
    # Predictions
    y_pred = ensemble_predict(ensemble_models, X_test)

    # Classification report
    print(f"\n=== {name} Ensemble: Classification Report ===")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    print(f"\n=== {name} Ensemble: Confusion Matrix ===")
    print(confusion_matrix(y_test, y_pred))

    # ROC-AUC (use predicted probabilities)
    y_probas = np.mean([m.predict_proba(X_test)[:, 1] for m in ensemble_models], axis=0)
    auc_score = roc_auc_score(y_test, y_probas)
    print(f"\n{name} Ensemble: ROC-AUC = {auc_score:.4f}")

    # Plot ROC curve
    if plot:
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.figure(figsize=(7,5))
        plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color="blue")
        plt.plot([0, 1], [0, 1], "k--", label="Chance")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve - {name} Ensemble")
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()



# ----------------------------
# Train-Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# Random Forest Ensemble
# ----------------------------
rf_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=RandomForestClassifier()
)
evaluate_ensemble("Random Forest", rf_models, X_test, y_test)


# ----------------------------
# Logistic Regression Ensemble
# ----------------------------
lr_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=LogisticRegression(max_iter=1000)
)
evaluate_ensemble("Logistic Regression", lr_models, X_test, y_test)


# ----------------------------
# XGBoost Ensemble
# ----------------------------
xgb_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=XGBClassifier(
        eval_metric="logloss"
    )
)
evaluate_ensemble("XGBoost", xgb_models, X_test, y_test)





import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target column
target = "GLOF"   # change if yours is "Activity" or another name

# Drop unwanted + target
exclude_cols = ["Year_final", "Latitude", "Longitude","Elevation_m", target]
X = df.drop(columns=exclude_cols)
y = df[target]

# Separate categorical & numeric columns
categorical_cols = ["Lake_type_simplified"]
numeric_cols = [col for col in X.columns if col not in categorical_cols]

# Keep only complete rows (CCA subset)
X_cca = X.dropna()

print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%)
# ==============================================
X_masked = X_cca.copy()
rng = np.random.default_rng(42)

# Mask numeric columns
for col in numeric_cols:
    mask = rng.uniform(size=X_cca.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# Mask categorical column
for col in categorical_cols:
    mask = rng.uniform(size=X_cca.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# ==============================================
# 3. Define preprocessors
# ==============================================
numeric_transformer = Pipeline(steps=[
    ("imputer", IterativeImputer(random_state=42, max_iter=10))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols)
    ]
)

# ==============================================
# 4. Fit & Transform (Impute)
# ==============================================
X_imputed = preprocessor.fit_transform(X_masked)

# Rebuild imputed dataframe
imputed_feature_names = (
    numeric_cols +
    list(preprocessor.named_transformers_["cat"]["onehot"].get_feature_names_out(categorical_cols))
)
X_imputed = pd.DataFrame(X_imputed, columns=imputed_feature_names, index=X_masked.index)

# ==============================================
# 5. Evaluate Imputation Accuracy
# ==============================================
print("\n===Iterative Imputer==")
print("\n=== Numeric RMSE ===")
for col in numeric_cols:
    missing_mask = X_masked[col].isna()
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col]
        imputed_vals = X_imputed.loc[missing_mask, col]
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy ===")
for col in categorical_cols:
    missing_mask = X_masked[col].isna()
    if missing_mask.any():
        # Get the imputed column set for this categorical
        onehot_cols = [c for c in X_imputed.columns if c.startswith(col + "_")]
        imputed_cat = X_imputed.loc[missing_mask, onehot_cols].idxmax(axis=1).str.replace(col + "_", "")
        true_cat = X_cca.loc[missing_mask, col].astype(str)
        acc = accuracy_score(true_cat, imputed_cat)
        print(f"{col}: Accuracy = {acc:.3f}")










pip install miceforest






import pandas as pd
import numpy as np
import miceforest as mf
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only
X = df.drop(columns=exclude_cols)

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

# Keep only complete rows (CCA subset) + reset index
X_cca = X.dropna().reset_index(drop=True)
print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%)
# ==============================================
X_masked_manual = X_cca.copy()
rng = np.random.default_rng(42)

for col in X_masked_manual.columns:
    mask = rng.uniform(size=X_masked_manual.shape[0]) < 0.2
    X_masked_manual.loc[mask, col] = np.nan

# ==============================================
# 3. Run miceforest Imputation
# ==============================================
kernel_manual = mf.ImputationKernel(
    X_masked_manual,
    save_all_iterations_data=True,
    random_state=42
)

kernel_manual.mice(iterations=10, n_datasets=5)

X_imputed_manual = kernel_manual.complete_data(
    dataset=0,
    iteration=kernel_manual.iteration_count() - 1
)

# ==============================================
# 4. Evaluate Imputation Accuracy
# ==============================================
print("\n=== Numeric RMSE (Manual) ===")
for col in X_cca.select_dtypes(include=[np.number]).columns:
    missing_mask = X_masked_manual[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].values
        imputed_vals = X_imputed_manual.loc[missing_mask, col].values
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy (Manual) ===")
for col in X_cca.select_dtypes(include=["category"]).columns:
    missing_mask = X_masked_manual[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].astype(str).values
        imputed_vals = X_imputed_manual.loc[missing_mask, col].astype(str).values
        acc = accuracy_score(true_vals, imputed_vals)
        print(f"{col}: Accuracy = {acc:.3f}")








import pandas as pd
import numpy as np
import miceforest as mf
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only
X = df.drop(columns=exclude_cols)

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

# Keep only complete rows (CCA subset) + reset index
X_cca = X.dropna().reset_index(drop=True)
print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%) with ampute_data
# ==============================================
X_masked_ampute = mf.ampute_data(
    X_cca,
    perc=0.2,
    random_state=42
)

# ==============================================
# 3. Run miceforest Imputation
# ==============================================
kernel_ampute = mf.ImputationKernel(
    X_masked_ampute,
    save_all_iterations_data=True,
    random_state=42
)
kernel_ampute.mice(iterations=10, n_datasets=5)

X_imputed_ampute = kernel_ampute.complete_data(
    dataset=0,
    iteration=kernel_ampute.iteration_count() - 1
)

# ==============================================
# 4. Evaluate Imputation Accuracy
# ==============================================
print("\n=== Numeric RMSE (Ampute) ===")
for col in X_cca.select_dtypes(include=[np.number]).columns:
    missing_mask = X_masked_ampute[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].values
        imputed_vals = X_imputed_ampute.loc[missing_mask, col].values
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy (Ampute) ===")
for col in X_cca.select_dtypes(include=["category"]).columns:
    missing_mask = X_masked_ampute[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].astype(str).values
        imputed_vals = X_imputed_ampute.loc[missing_mask, col].astype(str).values
        acc = accuracy_score(true_vals, imputed_vals)
        print(f"{col}: Accuracy = {acc:.3f}")



#install plotnine
!pip install plotnine --quiet


# Plot observed vs imputed distributions: Manual Masking
kernel_manual.plot_imputed_distributions()



# Plot observed vs imputed distributions: Ampute Masking
!pip install plotnine --quiet
kernel_ampute.plot_imputed_distributions()


import seaborn as sns
import matplotlib.pyplot as plt

def compare_kde(col, X_true, X_imp_manual=None, X_imp_ampute=None):
    plt.figure(figsize=(8,5))

    # True distribution (from CCA complete cases)
    sns.kdeplot(X_true[col], label="True (Complete Cases)", color="black", lw=2)

    # Manual masking imputations
    if X_imp_manual is not None:
        sns.kdeplot(X_imp_manual[col], label="Imputed (Manual)", color="red", lw=2)

    # Ampute masking imputations
    if X_imp_ampute is not None:
        sns.kdeplot(X_imp_ampute[col], label="Imputed (Ampute)", color="blue", lw=2)

    plt.title(f"Distribution Comparison: {col}")
    plt.xlabel(col)
    plt.ylabel("Density")
    plt.legend()
    plt.show()




# Compare expansion rate distributions
compare_kde("5y_expansion_rate", X_cca, X_imputed_manual, X_imputed_ampute)
compare_kde("10y_expansion_rate", X_cca, X_imputed_manual, X_imputed_ampute)

# Compare elevation distributions
compare_kde("Elevation_m", X_cca, X_imputed_manual, X_imputed_ampute)



# Basic statistics for each numeric column
desc = X_cca.describe().T  # mean, std, min, max, quartiles
# Add extra columns for range and coefficient of variation
desc["range"] = desc["max"] - desc["min"]
desc["cv"] = desc["std"] / desc["mean"]  # coefficient of variation
print(desc[["mean", "std", "min", "25%", "50%", "75%", "max", "range", "cv"]])



X_imputed


# Add target back
final_df = X_imputed.copy()
final_df[target] = df.loc[X_imputed.index, target].values
final_df







# ==============================================
# Imputation with MICE (miceforest) on full dataset
# With imputation flags for key variables
# ==============================================

import pandas as pd
import numpy as np
import miceforest as mf

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only (excluding target + year)
X = df.drop(columns=exclude_cols).copy()

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

print(f"Original shape: {X.shape}")
print("Missing values (%):")
print(X.isnull().mean() * 100)

# ==============================================
# 2. Run miceforest Imputation (real missingness)
# ==============================================
kernel = mf.ImputationKernel(
    X,
    num_datasets=5,                # <-- Correct parameter
    save_all_iterations_data=True,
    random_state=42
)

# Run MICE: 10 iterations
kernel.mice(iterations=10)

# ==============================================
# 3. Retrieve ALL imputed datasets + add flags
# ==============================================
# Variables to flag (based on your missingness analysis)
flag_vars = [
    "Lake_area_calculated_ha",
    "5y_expansion_rate",
    "10y_expansion_rate",
    "glacier_area_ha",
    "slope_glac_to_lake",
    "nearest_glacier_dist_m",
    "glacier_elev_m",
]

# Store finished datasets in a dictionary
X_imputed_dict = {}
last_iter = kernel.iteration_count() - 1

for d in range(5):   # loop over 0–4
    X_imp = kernel.complete_data(dataset=d, iteration=last_iter).copy()
    
    # Add imputation flags
    for col in flag_vars:
        X_imp[f"is_imputed_{col}"] = X[col].isna().astype(int)
    
    # Reattach target
    X_imp[target] = df[target].values
    
    # Save to dictionary
    X_imputed_dict[d] = X_imp
    print(f"Dataset {d} shape: {X_imp.shape}")

# Example: access dataset 0
X_imputed_0 = X_imputed_dict[0]
print("\nFinal dataset 0 shape:", X_imputed_0.shape)




# ==============================================
# Imputation with MICE (miceforest) on full dataset
# With imputation flags for key variables
# ==============================================

import pandas as pd
import numpy as np
import miceforest as mf

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only (excluding target + year)
X = df.drop(columns=exclude_cols).copy()

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

print(f"Original shape: {X.shape}")
print("Missing values (%):")
print(X.isnull().mean() * 100)

# ==============================================
# 2. Run miceforest Imputation (real missingness)
# ==============================================
kernel = mf.ImputationKernel(
    X,
    num_datasets=5,                # <-- Correct parameter
    save_all_iterations_data=True,
    random_state=42
)

# Run MICE: 10 iterations
kernel.mice(iterations=10)

# ==============================================
# 3. Retrieve ALL imputed datasets + add flags
# ==============================================
# Variables to flag (based on your missingness analysis)
flag_vars = [
    "Lake_area_calculated_ha",
    "5y_expansion_rate",
    "10y_expansion_rate",
    "glacier_area_ha",
    "slope_glac_to_lake",
    "nearest_glacier_dist_m",
    "glacier_elev_m",
]

# Store finished datasets in a dictionary
X_imputed_dict = {}
last_iter = kernel.iteration_count() - 1

for d in range(5):   # loop over 0–4
    X_imp = kernel.complete_data(dataset=d, iteration=last_iter).copy()
    
    # Add imputation flags
    for col in flag_vars:
        X_imp[f"is_imputed_{col}"] = X[col].isna().astype(int)
    
    # Reattach target
    X_imp[target] = df[target].values
    
    # Save to dictionary
    X_imputed_dict[d] = X_imp
    print(f"Dataset {d} shape: {X_imp.shape}")

# Example: access dataset 0
X_imputed_0 = X_imputed_dict[0]
print("\nFinal dataset 0 shape:", X_imputed_0.shape)



#Accessing the datasets:
X_imputed_0 = X_imputed_dict[0]
X_imputed_1 = X_imputed_dict[1]
X_imputed_2 = X_imputed_dict[2]
X_imputed_3 = X_imputed_dict[3]
X_imputed_4 = X_imputed_dict[4]









# ==============================================
# MICE-Imputed Pipeline Setup
# ==============================================

# Features and Target
X_MICE = X_imputed_2.drop("GLOF", axis=1)   # <- df_MICE is your completed dataset from MICE
y_MICE = X_imputed_2["GLOF"]

# Define categorical + numerical features
cat_features = ["Lake_type_simplified"]
exclude_cols = cat_features + ["Year_final", "Latitude", "Longitude"]
num_features = [col for col in X_MICE.columns if col not in exclude_cols]

# Preprocessing pipeline
preprocessor_MICE = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features)
    ]
)

# Example: attach to a classifier (e.g., Random Forest)
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

pipeline_MICE = Pipeline(
    steps=[
        ("preprocessor", preprocessor_MICE),
        ("classifier", RandomForestClassifier(random_state=42,class_weight="balanced"))
    ]
)




from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import numpy as np

# ===================================================
# Helper function to evaluate a model
# ===================================================
def evaluate_model(name, model, X_train, X_test, y_train, y_test, preprocessor):
    # Build pipeline
    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])
    
    # Fit
    pipeline.fit(X_train, y_train)
    
    # Predict
    y_pred = pipeline.predict(X_test)
    y_probas = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None
    
    # Classification Report
    print(f"\n=== {name} ===")
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    # ROC-AUC
    if y_probas is not None:
        auc_score = roc_auc_score(y_test, y_probas)
        print(f"ROC-AUC = {auc_score:.4f}")
        
        # Plot ROC
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc_score:.2f})")

# ===================================================
# Train-test split
# ===================================================
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_MICE, y_MICE, test_size=0.2, random_state=42, stratify=y_MICE
)

# ===================================================
# Try multiple classifiers
# ===================================================
plt.figure(figsize=(7,5))

evaluate_model("Logistic Regression", LogisticRegression(max_iter=1000, class_weight="balanced"),
               X_train, X_test, y_train, y_test, preprocessor_MICE)

evaluate_model("Random Forest", RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42),
               X_train, X_test, y_train, y_test, preprocessor_MICE)

evaluate_model("Gradient Boosting", GradientBoostingClassifier(),
               X_train, X_test, y_train, y_test, preprocessor_MICE)

evaluate_model("SVM (Linear)", SVC(probability=True, kernel="linear", class_weight="balanced", random_state=42),
               X_train, X_test, y_train, y_test, preprocessor_MICE)

plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves Across Models - MICE Imputation")
plt.legend()
plt.grid(True)
plt.show()



from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, roc_curve, precision_recall_curve
)
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# ===================================================
#  Put your imputed datasets in a list
# ===================================================
imputed_datasets = [X_imputed_0, X_imputed_1, X_imputed_2, X_imputed_3, X_imputed_4]

# Each dataset needs y as well (same for all)
y_list = [y for _ in imputed_datasets]

# ===================================================
#  Evaluate Logistic Regression across imputations
# ===================================================
def evaluate_across_imputations(imputed_datasets, y_list, preprocessor, threshold=0.5):
    probs_all = []
    aucs = []

    for i, (X_imp, y_imp) in enumerate(zip(imputed_datasets, y_list)):
        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X_imp, y_imp, test_size=0.2, random_state=42, stratify=y_imp
        )

        # Logistic Regression (balanced for recall)
        clf = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", LogisticRegression(max_iter=1000, class_weight="balanced"))
        ])

        clf.fit(X_train, y_train)

        # Predicted probabilities
        y_probas = clf.predict_proba(X_test)[:, 1]
        probs_all.append(y_probas)

        # AUC per imputation
        aucs.append(roc_auc_score(y_test, y_probas))

    # ==============================================
    # Pooled Predictions (average probs)
    # ==============================================
    probs_mean = np.mean(probs_all, axis=0)
    y_pred_pooled = (probs_mean >= threshold).astype(int)

    print("\n=== Logistic Regression (Pooled over 5 MICE Imputations) ===")
    print(classification_report(y_test, y_pred_pooled))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred_pooled))
    print(f"ROC-AUC (pooled probs): {roc_auc_score(y_test, probs_mean):.4f}")
    print(f"AUC mean±std across imputations: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}")

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test, probs_mean)
    plt.figure(figsize=(7,5))
    plt.plot(fpr, tpr, label=f"Pooled AUC = {roc_auc_score(y_test, probs_mean):.2f}", color="blue")
    plt.plot([0,1],[0,1],"k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve - Logistic Regression (5x MICE Imputations)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Precision-Recall curve
    precisions, recalls, thresholds = precision_recall_curve(y_test, probs_mean)
    plt.figure(figsize=(7,5))
    plt.plot(thresholds, recalls[:-1], label="Recall", color="green")
    plt.plot(thresholds, precisions[:-1], label="Precision", color="red")
    plt.xlabel("Decision Threshold")
    plt.ylabel("Score")
    plt.title("Precision-Recall Tradeoff (Logistic Regression, Pooled)")
    plt.legend()
    plt.grid(True)
    plt.show()

# ===================================================
# Run the pooled evaluation
# ===================================================
evaluate_across_imputations(imputed_datasets, y_list, preprocessor_MICE, threshold=0.5)



from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import recall_score, roc_auc_score, roc_curve
import numpy as np
import matplotlib.pyplot as plt

def cv_metrics_across_imputations(imputed_datasets, y, preprocessor, folds=10):
    recalls, aucs = [], []
    all_probs, all_true = [], []

    for i, X_imp in enumerate(imputed_datasets):
        print(f"\n=== Imputation {i} ===")
        cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)

        # Pipeline with Logistic Regression
        pipeline = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", LogisticRegression(max_iter=1000, class_weight="balanced"))
        ])

        # CV recall
        recall_scores = cross_val_score(
            pipeline, X_imp, y, cv=cv, scoring="recall"
        )
        print("Recall per fold:", recall_scores)
        print("Mean Recall:", np.mean(recall_scores), "±", np.std(recall_scores))
        recalls.extend(recall_scores)

        # Out-of-fold probabilities for ROC
        y_probas = cross_val_predict(
            pipeline, X_imp, y, cv=cv, method="predict_proba"
        )[:, 1]
        auc = roc_auc_score(y, y_probas)
        aucs.append(auc)

        all_probs.append(y_probas)
        all_true.append(y)

    # ========================================
    # Pool across imputations
    # ========================================
    print("\n=== Pooled Across 5 Imputations ===")
    print(f"Recall: {np.mean(recalls):.3f} ± {np.std(recalls):.3f}")
    print(f"AUC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}")

    # ROC curve with pooled probs (average across imputations)
    mean_probs = np.mean(all_probs, axis=0)
    fpr, tpr, _ = roc_curve(all_true[0], mean_probs)

    plt.figure(figsize=(7,5))
    plt.plot(fpr, tpr, label=f"Mean ROC (AUC={np.mean(aucs):.2f})", color="blue")
    plt.plot([0,1],[0,1],"k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Pooled ROC Curve (Logistic Regression, 5x MICE Imputations)")
    plt.legend()
    plt.grid(True)
    plt.show()

    return recalls, aucs



from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    recall_score, precision_score, f1_score, roc_auc_score, roc_curve
)
import numpy as np
import matplotlib.pyplot as plt

def evaluate_cv_all_imputations(imputed_datasets, y, preprocessor, folds=10):
    all_metrics = []

    for i, X_imp in enumerate(imputed_datasets):
        print(f"\n=== Imputation {i} ===")
        cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)

        # Pipeline
        pipeline = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", LogisticRegression(max_iter=1000, class_weight="balanced"))
        ])

        # Get out-of-fold predictions
        y_probas = cross_val_predict(
            pipeline, X_imp, y, cv=cv, method="predict_proba"
        )[:, 1]
        y_pred = (y_probas >= 0.5).astype(int)

        # Metrics
        recall = recall_score(y, y_pred)
        precision = precision_score(y, y_pred)
        f1 = f1_score(y, y_pred)
        auc = roc_auc_score(y, y_probas)

        print(f"Recall: {recall:.3f}, Precision: {precision:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}")
        all_metrics.append([recall, precision, f1, auc, y_probas])

    # =========================================
    # Pool across imputations
    # =========================================
    recalls = [m[0] for m in all_metrics]
    precisions = [m[1] for m in all_metrics]
    f1s = [m[2] for m in all_metrics]
    aucs = [m[3] for m in all_metrics]
    probas_all = np.mean([m[4] for m in all_metrics], axis=0)

    print("\n=== Pooled Across 5 Imputations ===")
    print(f"Recall: {np.mean(recalls):.3f} ± {np.std(recalls):.3f}")
    print(f"Precision: {np.mean(precisions):.3f} ± {np.std(precisions):.3f}")
    print(f"F1-score: {np.mean(f1s):.3f} ± {np.std(f1s):.3f}")
    print(f"AUC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}")

    # ROC curve
    fpr, tpr, _ = roc_curve(y, probas_all)
    plt.figure(figsize=(7,5))
    plt.plot(fpr, tpr, label=f"Pooled AUC={roc_auc_score(y, probas_all):.2f}", color="blue")
    plt.plot([0,1],[0,1],"k--")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Pooled ROC Curve - Logistic Regression (5x MICE Imputations)")
    plt.legend()
    plt.grid(True)
    plt.show()

# ======================================
# Run on your datasets
# ======================================
imputed_datasets = [X_imputed_0, X_imputed_1, X_imputed_2, X_imputed_3, X_imputed_4]
evaluate_cv_all_imputations(imputed_datasets, y, preprocessor_MICE, folds=10)



from sklearn.utils import resample
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def easy_ensemble(X, y, preprocessor, n_subsets=5, model=None, threshold=0.5):
    """
    EasyEnsemble with random undersampling of majority class.
    
    Args:
        X (pd.DataFrame): Features with column names
        y (pd.Series): Target (0/1)
        preprocessor: ColumnTransformer
        n_subsets (int): Number of undersampled balanced sets
        model: Base model (default LogisticRegression)
        threshold (float): Classification threshold
    
    Returns:
        y_pred (np.array): Final binary predictions
        probs_mean (np.array): Averaged probabilities
    """
    if model is None:
        model = LogisticRegression(max_iter=1000)

    probs_all = []

    for i in range(n_subsets):
        # Split minority and majority (as DataFrames/Series)
        X_min = X[y==1]
        y_min = y[y==1]
        X_maj = X[y==0]
        y_maj = y[y==0]

        # Undersample majority to match minority
        X_maj_sample, y_maj_sample = resample(
            X_maj, y_maj,
            replace=False,
            n_samples=len(y_min),
            random_state=42+i
        )

        # Combine balanced dataset (keep as DataFrame)
        X_bal = pd.concat([X_min, X_maj_sample])
        y_bal = pd.concat([y_min, y_maj_sample])

        # Train pipeline
        pipeline = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", model)
        ])
        pipeline.fit(X_bal, y_bal)

        # Predict probs on full dataset
        probs = pipeline.predict_proba(X)[:,1]
        probs_all.append(probs)

    # Average predictions
    probs_mean = np.mean(probs_all, axis=0)
    y_pred = (probs_mean >= threshold).astype(int)

    # Metrics
    auc = roc_auc_score(y, probs_mean)
    print("\n=== EasyEnsemble (Undersampling + Model) ===")
    print(classification_report(y, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y, y_pred))
    print(f"ROC-AUC: {auc:.3f}")

    # ROC Curve
    fpr, tpr, _ = roc_curve(y, probs_mean)
    plt.figure(figsize=(7,5))
    plt.plot(fpr, tpr, label=f"AUC={auc:.2f}", color="blue")
    plt.plot([0,1],[0,1],'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve - EasyEnsemble")
    plt.legend()
    plt.grid(True)
    plt.show()

    return y_pred, probs_mean



# Run EasyEnsemble with Logistic Regression
y_pred, probs_mean, metrics = easy_ensemble(
    X_imputed_0, y, preprocessor_MICE,
    n_subsets=5,
    model=LogisticRegression(max_iter=1000)
)

# Attach probabilities back to your dataframe
df_with_probs = df_comb.copy()
df_with_probs["EasyEnsemble_Prob"] = probs_mean



import numpy as np
from sklearn.metrics import precision_recall_curve

def find_best_threshold(y_true, y_probas, recall_target=None, precision_target=None):
    """
    Find decision threshold based on desired recall or precision.
    
    Args:
        y_true (array): True labels (0/1)
        y_probas (array): Predicted probabilities for class 1
        recall_target (float): Desired minimum recall (0–1)
        precision_target (float): Desired minimum precision (0–1)
    
    Returns:
        threshold (float): Best threshold found
        precision (float): Precision at that threshold
        recall (float): Recall at that threshold
    """
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_probas)
    
    best_idx = None
    
    if recall_target is not None:
        # Find lowest threshold that achieves at least recall_target
        candidates = np.where(recalls >= recall_target)[0]
        if len(candidates) == 0:
            raise ValueError(f"No threshold found with recall ≥ {recall_target}")
        best_idx = candidates[-1]  # last one to maximize precision as well
    
    elif precision_target is not None:
        # Find highest threshold that achieves at least precision_target
        candidates = np.where(precisions >= precision_target)[0]
        if len(candidates) == 0:
            raise ValueError(f"No threshold found with precision ≥ {precision_target}")
        best_idx = candidates[0]
    
    else:
        raise ValueError("Must specify either recall_target or precision_target.")
    
    threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    return threshold, precisions[best_idx], recalls[best_idx]



# Step 1: Run EasyEnsemble to get probabilities
y_pred, probs_mean = easy_ensemble(X_imputed_0, y, preprocessor_MICE)

# Step 2: Choose thresholds
t, p, r = find_best_threshold(y, probs_mean, recall_target=0.90)
print(f"Threshold for Recall≥0.90: {t:.2f} | Precision={p:.2f}, Recall={r:.2f}")

t, p, r = find_best_threshold(y, probs_mean, precision_target=0.80)
print(f"Threshold for Precision≥0.80: {t:.2f} | Precision={p:.2f}, Recall={r:.2f}")




X_imputed_0.to_csv("X_imputed_0.csv",index=False)









# ==============================================
# Imputation with MICE (miceforest) on full dataset
# With imputation flags for key variables
# ==============================================

import pandas as pd
import numpy as np
import miceforest as mf

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only (excluding target + year)
X = df.drop(columns=exclude_cols).copy()

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

print(f"Original shape: {X.shape}")
print("Missing values (%):")
print(X.isnull().mean() * 100)

# ==============================================
# 2. Run miceforest Imputation (real missingness)
# ==============================================
kernel = mf.ImputationKernel(
    X,
    num_datasets=5,                # <-- Correct parameter
    save_all_iterations_data=True,
    random_state=42
)

# Run MICE: 10 iterations
kernel.mice(iterations=10)

# ==============================================
# 3. Retrieve ALL imputed datasets + add flags
# ==============================================
# Variables to flag (based on your missingness analysis)
flag_vars = [
    "Lake_area_calculated_ha",
    "5y_expansion_rate",
    "10y_expansion_rate",
    "glacier_area_ha",
    "slope_glac_to_lake",
    "nearest_glacier_dist_m",
    "glacier_elev_m",
]

# Store finished datasets in a dictionary
X_imputed_dict = {}
last_iter = kernel.iteration_count() - 1

for d in range(5):   # loop over 0–4
    X_imp = kernel.complete_data(dataset=d, iteration=last_iter).copy()
    
    # Add imputation flags
    for col in flag_vars:
        X_imp[f"is_imputed_{col}"] = X[col].isna().astype(int)
    
    # Reattach target
    X_imp[target] = df[target].values
    
    # Save to dictionary
    X_imputed_dict[d] = X_imp
    print(f"Dataset {d} shape: {X_imp.shape}")

# Example: access dataset 0
X_imputed_0 = X_imputed_dict[0]
print("\nFinal dataset 0 shape:", X_imputed_0.shape)



# ==============================================
# 4. Train/Test Split + Encoding + Random Forest Evaluation
# ==============================================

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import numpy as np

# Use imputed dataset 0
X_imp = X_imputed_dict[0].copy()

# Separate features and target
X_final = X_imp.drop(columns=[target])
y_final = X_imp[target]

# Identify categorical vs numeric columns
cat_cols = X_final.select_dtypes(include=["object", "category"]).columns.tolist()
num_cols = X_final.select_dtypes(include=[np.number]).columns.tolist()

print("Categorical columns:", cat_cols)
print("Numeric columns:", num_cols[:5], "...")  # show first few

# ==============================================
# 5. Train/Test Split
# ==============================================
X_train, X_test, y_train, y_test = train_test_split(
    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final
)

# ==============================================
# 6. Preprocessing + Random Forest Pipeline
# ==============================================
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
        ("num", "passthrough", num_cols)
    ]
)

rf = RandomForestClassifier(
    n_estimators=300,
    random_state=42,
    class_weight="balanced",
    n_jobs=-1
)

# Combine preprocessing + model
pipe = Pipeline(steps=[("preprocessor", preprocessor), ("model", rf)])

# Train the model
pipe.fit(X_train, y_train)

# ==============================================
# 7. ROC Curve and AUC
# ==============================================
y_pred_prob = pipe.predict_proba(X_test)[:, 1]
y_pred = pipe.predict(X_test)

auc = roc_auc_score(y_test, y_pred_prob)
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)

plt.figure(figsize=(7,6))
plt.plot(fpr, tpr, color="blue", lw=2, label=f"Random Forest (AUC = {auc:.3f})")
plt.plot([0, 1], [0, 1], color="grey", lw=1, linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Random Forest (MICE Imputed Data)")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

print(f"ROC-AUC Score: {auc:.4f}")

# ==============================================
# 8. Confusion Matrix
# ==============================================
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix - Random Forest (MICE Imputed Data)")
plt.show()



# Classification report (per class metrics)
print("Detailed Classification Report:")
print(classification_report(y_test, y_pred, digits=3))


# ==============================================
# Precision–Recall Tradeoff + Custom Threshold
# ==============================================

from sklearn.metrics import precision_recall_curve

# Get predicted probabilities for class 1 (GLOF)
y_probs = pipe.predict_proba(X_test)[:, 1]

# Compute precision–recall curve
precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)

plt.figure(figsize=(7,6))
plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
plt.plot(thresholds, recalls[:-1], "g-", label="Recall")
plt.xlabel("Decision Threshold")
plt.ylabel("Score")
plt.title("Precision–Recall Tradeoff (Random Forest)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()







